{
  "results": {
    "hendrycksTest-international_law": {
      "acc": 0.6033057851239669,
      "acc_stderr": 0.044658697805310094,
      "acc_norm": 0.6611570247933884,
      "acc_norm_stderr": 0.043207678075366705
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.5492227979274611,
      "acc_stderr": 0.035909109522355244,
      "acc_norm": 0.48704663212435234,
      "acc_norm_stderr": 0.036072280610477486
    },
    "truthfulqa_mc_mt_es": {
      "mc1": 0.27629911280101394,
      "mc1_stderr": 0.015929648357222322,
      "mc2": 0.4428962758833131,
      "mc2_stderr": 0.016018652897749865
    },
    "arc_challenge_mt_fr": {
      "acc": 0.25748502994011974,
      "acc_stderr": 0.012794024494042345,
      "acc_norm": 0.2917023096663815,
      "acc_norm_stderr": 0.013300146519228456
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.03794012674697031,
      "acc_norm": 0.4277456647398844,
      "acc_norm_stderr": 0.037724468575180276
    },
    "arc_easy": {
      "acc": 0.7988215488215489,
      "acc_stderr": 0.00822590726296517,
      "acc_norm": 0.7824074074074074,
      "acc_norm_stderr": 0.008466554789073771
    },
    "hellaswag_mt_pt": {
      "acc": 0.2969985913966844,
      "acc_stderr": 0.0047566533161405525,
      "acc_norm": 0.3456495828367104,
      "acc_norm_stderr": 0.0049507320033545
    },
    "hendrycksTest-nutrition": {
      "acc": 0.46078431372549017,
      "acc_stderr": 0.028541722692618874,
      "acc_norm": 0.47058823529411764,
      "acc_norm_stderr": 0.028580341065138293
    },
    "piqa": {
      "acc": 0.7878128400435256,
      "acc_stderr": 0.009539299828174046,
      "acc_norm": 0.7921653971708379,
      "acc_norm_stderr": 0.009466997964536414
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.0347327959083696,
      "acc_norm": 0.5252525252525253,
      "acc_norm_stderr": 0.03557806245087314
    },
    "hellaswag": {
      "acc": 0.5588528181637125,
      "acc_stderr": 0.004955095096264713,
      "acc_norm": 0.7381995618402709,
      "acc_norm_stderr": 0.004387161203087965
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.04769300568972745,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.0469237132203465
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.026992199173064356,
      "acc_norm": 0.2624113475177305,
      "acc_norm_stderr": 0.026244920349843007
    },
    "arc_challenge_mt_pt": {
      "acc": 0.23247863247863249,
      "acc_stderr": 0.012354623157402241,
      "acc_norm": 0.26153846153846155,
      "acc_norm_stderr": 0.012853590720803446
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.05021167315686781
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.19,
      "acc_stderr": 0.03942772444036622,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-management": {
      "acc": 0.6601941747572816,
      "acc_stderr": 0.046897659372781335,
      "acc_norm": 0.5825242718446602,
      "acc_norm_stderr": 0.048828405482122375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4473684210526316,
      "acc_stderr": 0.04046336883978251,
      "acc_norm": 0.45394736842105265,
      "acc_norm_stderr": 0.04051646342874141
    },
    "truthfulqa_mc_mt_nl": {
      "mc1": 0.22802547770700637,
      "mc1_stderr": 0.014984254458017715,
      "mc2": 0.3967006569473872,
      "mc2_stderr": 0.0160302027417862
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4957983193277311,
      "acc_stderr": 0.032477343344481116,
      "acc_norm": 0.46218487394957986,
      "acc_norm_stderr": 0.032385469487589795
    },
    "hellaswag_mt_it": {
      "acc": 0.28641357554661157,
      "acc_stderr": 0.004715362455914123,
      "acc_norm": 0.321005112585663,
      "acc_norm_stderr": 0.00486949830454679
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4803921568627451,
      "acc_stderr": 0.03506612560524866,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.03460228327239172
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3310344827586207,
      "acc_stderr": 0.03921545312467122,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.039609335494512087
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.042446332383532286,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.04188307537595853
    },
    "hendrycksTest-virology": {
      "acc": 0.45180722891566266,
      "acc_stderr": 0.03874371556587953,
      "acc_norm": 0.3795180722891566,
      "acc_norm_stderr": 0.03777798822748018
    },
    "lambada_openai_mt_es": {
      "ppl": 443.6209374448513,
      "ppl_stderr": 30.192255632181652,
      "acc": 0.22957500485154278,
      "acc_stderr": 0.005859216640699744
    },
    "lambada_openai_mt_it": {
      "ppl": 875.8038385804047,
      "ppl_stderr": 63.804018119711294,
      "acc": 0.21831942557733358,
      "acc_stderr": 0.005755365677563655
    },
    "truthfulqa_mc_mt_pt": {
      "mc1": 0.2982233502538071,
      "mc1_stderr": 0.016307332001990155,
      "mc2": 0.4627712710193506,
      "mc2_stderr": 0.01629127874284062
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest_mt_it": {
      "acc": 0.29362880886426596,
      "acc_stderr": 0.011988991585787666,
      "acc_norm": 0.2832409972299169,
      "acc_norm_stderr": 0.011861277597694286
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4722222222222222,
      "acc_stderr": 0.04174752578923185,
      "acc_norm": 0.4305555555555556,
      "acc_norm_stderr": 0.04140685639111503
    },
    "arc_challenge_mt_es": {
      "acc": 0.2641025641025641,
      "acc_stderr": 0.01289400095710378,
      "acc_norm": 0.3094017094017094,
      "acc_norm_stderr": 0.01351969357284718
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4007352941176471,
      "acc_stderr": 0.029768263528933116,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.02806499816704009
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5516129032258065,
      "acc_stderr": 0.02829205683011274,
      "acc_norm": 0.5225806451612903,
      "acc_norm_stderr": 0.028414985019707868
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3122555410691004,
      "acc_stderr": 0.01183579813568317,
      "acc_norm": 0.3116036505867014,
      "acc_norm_stderr": 0.011829039182849643
    },
    "hendrycksTest_mt_es": {
      "acc": 0.29146757679180885,
      "acc_stderr": 0.011876936956522324,
      "acc_norm": 0.27986348122866894,
      "acc_norm_stderr": 0.011733024975278845
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.49696969696969695,
      "acc_stderr": 0.03904272341431857,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.03903698647748441
    },
    "hellaswag_mt_nl": {
      "acc": 0.2743658931462493,
      "acc_stderr": 0.004635799367478044,
      "acc_norm": 0.2974635725849973,
      "acc_norm_stderr": 0.004749545423775578
    },
    "sciq": {
      "acc": 0.949,
      "acc_stderr": 0.006960420062571408,
      "acc_norm": 0.927,
      "acc_norm_stderr": 0.008230354715244066
    },
    "hendrycksTest-marketing": {
      "acc": 0.7264957264957265,
      "acc_stderr": 0.029202540153431187,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.031937057262002924
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.49056603773584906,
      "acc_stderr": 0.0307673947078081,
      "acc_norm": 0.4528301886792453,
      "acc_norm_stderr": 0.03063562795796182
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562426,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.050161355804659205
    },
    "hellaswag_mt_fr": {
      "acc": 0.304776183336903,
      "acc_stderr": 0.004763752113416294,
      "acc_norm": 0.35146712358106663,
      "acc_norm_stderr": 0.004940884168157304
    },
    "truthfulqa_mc": {
      "mc1": 0.3108935128518972,
      "mc1_stderr": 0.016203316673559696,
      "mc2": 0.445079982888258,
      "mc2_stderr": 0.01512401299934772
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4641350210970464,
      "acc_stderr": 0.03246338898055659,
      "acc_norm": 0.4641350210970464,
      "acc_norm_stderr": 0.03246338898055659
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.4312169312169312,
      "acc_stderr": 0.025506481698138215,
      "acc_norm": 0.41005291005291006,
      "acc_norm_stderr": 0.025331202438944447
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3399014778325123,
      "acc_stderr": 0.033327690684107895,
      "acc_norm": 0.35960591133004927,
      "acc_norm_stderr": 0.03376458246509567
    },
    "hendrycksTest-sociology": {
      "acc": 0.4129353233830846,
      "acc_stderr": 0.03481520803367348,
      "acc_norm": 0.42786069651741293,
      "acc_norm_stderr": 0.03498541988407795
    },
    "hendrycksTest_mt_fr": {
      "acc": 0.2893436838390967,
      "acc_stderr": 0.012050503320170181,
      "acc_norm": 0.2865208186309104,
      "acc_norm_stderr": 0.012015369133952031
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3858520900321543,
      "acc_stderr": 0.027648149599751464,
      "acc_norm": 0.3987138263665595,
      "acc_norm_stderr": 0.0278093225857745
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3829787234042553,
      "acc_stderr": 0.03177821250236922,
      "acc_norm": 0.2936170212765957,
      "acc_norm_stderr": 0.02977164271249123
    },
    "lambada_openai_mt_de": {
      "ppl": 1087.2748554876136,
      "ppl_stderr": 74.89826534857788,
      "acc": 0.15990685037842034,
      "acc_stderr": 0.005106335304761904
    },
    "winogrande": {
      "acc": 0.7545382794001578,
      "acc_stderr": 0.012095272937183646
    },
    "truthfulqa_mc_mt_fr": {
      "mc1": 0.2617534942820839,
      "mc1_stderr": 0.015679624974143193,
      "mc2": 0.4110270241707255,
      "mc2_stderr": 0.015935091832741468
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.271523178807947,
      "acc_stderr": 0.036313298039696525,
      "acc_norm": 0.24503311258278146,
      "acc_norm_stderr": 0.03511807571804724
    },
    "arc_challenge_mt_it": {
      "acc": 0.21214713430282292,
      "acc_stderr": 0.011962438635487845,
      "acc_norm": 0.2309666381522669,
      "acc_norm_stderr": 0.012331780770152612
    },
    "hendrycksTest_mt_nl": {
      "acc": 0.2686671318911375,
      "acc_stderr": 0.011713678681309752,
      "acc_norm": 0.27424982554082344,
      "acc_norm_stderr": 0.011789496163308811
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.264804469273743,
      "acc_stderr": 0.014756906483260659,
      "acc_norm": 0.2782122905027933,
      "acc_norm_stderr": 0.014987325439963577
    },
    "lambada_openai": {
      "ppl": 5.557606101794971,
      "ppl_stderr": 0.15116445850985352,
      "acc": 0.6274015136813507,
      "acc_stderr": 0.006736051471289039
    },
    "hellaswag_mt_de": {
      "acc": 0.27487190435525194,
      "acc_stderr": 0.004612881325961329,
      "acc_norm": 0.3023057216054654,
      "acc_norm_stderr": 0.004745210733521547
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4567901234567901,
      "acc_stderr": 0.027716661650194038,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.02672586880910079
    },
    "hendrycksTest_mt_de": {
      "acc": 0.28778467908902694,
      "acc_stderr": 0.011897486200046149,
      "acc_norm": 0.2891649413388544,
      "acc_norm_stderr": 0.011914421424159924
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6146788990825688,
      "acc_stderr": 0.02086585085279413,
      "acc_norm": 0.5155963302752293,
      "acc_norm_stderr": 0.02142689153920805
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3988439306358382,
      "acc_stderr": 0.026362437574546545,
      "acc_norm": 0.36416184971098264,
      "acc_norm_stderr": 0.025906632631016113
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.2982456140350877,
      "acc_norm_stderr": 0.04303684033537315
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.44871794871794873,
      "acc_stderr": 0.025217315184846482,
      "acc_norm": 0.38461538461538464,
      "acc_norm_stderr": 0.02466674491518722
    },
    "truthfulqa_mc_mt_it": {
      "mc1": 0.24265644955300128,
      "mc1_stderr": 0.01532988894089986,
      "mc2": 0.39981825743266264,
      "mc2_stderr": 0.01623158397353943
    },
    "arc_challenge_mt_de": {
      "acc": 0.20444824636441403,
      "acc_stderr": 0.01180061094174358,
      "acc_norm": 0.24123182207014543,
      "acc_norm_stderr": 0.012518446323943841
    },
    "hendrycksTest_mt_pt": {
      "acc": 0.29599456890699255,
      "acc_stderr": 0.01189805496768126,
      "acc_norm": 0.2790224032586558,
      "acc_norm_stderr": 0.011690323180831802
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.026067159222275788,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.027940457136228412
    },
    "hendrycksTest-human_aging": {
      "acc": 0.452914798206278,
      "acc_stderr": 0.03340867501923325,
      "acc_norm": 0.3811659192825112,
      "acc_norm_stderr": 0.03259625118416827
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3425925925925926,
      "acc_stderr": 0.032365852526021574,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.03191923445686185
    },
    "arc_challenge": {
      "acc": 0.5307167235494881,
      "acc_stderr": 0.014583792546304037,
      "acc_norm": 0.5409556313993175,
      "acc_norm_stderr": 0.014562291073601233
    },
    "arc_challenge_mt_nl": {
      "acc": 0.18905047048759624,
      "acc_stderr": 0.011456827467940108,
      "acc_norm": 0.23438836612489308,
      "acc_norm_stderr": 0.01239512373569202
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.49074074074074076,
      "acc_stderr": 0.04832853553437055,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4215686274509804,
      "acc_stderr": 0.019977422600227467,
      "acc_norm": 0.38562091503267976,
      "acc_norm_stderr": 0.019691459052354154
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5343511450381679,
      "acc_stderr": 0.04374928560599738,
      "acc_norm": 0.3893129770992366,
      "acc_norm_stderr": 0.04276486542814591
    },
    "hellaswag_mt_es": {
      "acc": 0.31854064433539575,
      "acc_stderr": 0.004812415287412,
      "acc_norm": 0.3796671644975464,
      "acc_norm_stderr": 0.005012733265749145
    },
    "lambada_openai_mt_fr": {
      "ppl": 266.50840803426667,
      "ppl_stderr": 17.507523445922107,
      "acc": 0.28333009897147293,
      "acc_stderr": 0.006277952306965729
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.44642857142857145,
      "acc_stderr": 0.04718471485219588,
      "acc_norm": 0.4107142857142857,
      "acc_norm_stderr": 0.04669510663875191
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.4126984126984127,
      "acc_stderr": 0.04403438954768176,
      "acc_norm": 0.3492063492063492,
      "acc_norm_stderr": 0.04263906892795132
    },
    "truthfulqa_mc_mt_de": {
      "mc1": 0.24746192893401014,
      "mc1_stderr": 0.015382646812261827,
      "mc2": 0.43519761641071747,
      "mc2_stderr": 0.016156273021542784
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.49079754601226994,
      "acc_stderr": 0.03927705600787443,
      "acc_norm": 0.4110429447852761,
      "acc_norm_stderr": 0.038656978537853624
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5020408163265306,
      "acc_stderr": 0.0320089533497105,
      "acc_norm": 0.3551020408163265,
      "acc_norm_stderr": 0.03063565515038764
    },
    "hendrycksTest-computer_security": {
      "acc": 0.52,
      "acc_stderr": 0.05021167315686779,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6130268199233716,
      "acc_stderr": 0.017417138059440132,
      "acc_norm": 0.5057471264367817,
      "acc_norm_stderr": 0.01787878232612923
    },
    "hendrycksTest-college_physics": {
      "acc": 0.19607843137254902,
      "acc_stderr": 0.03950581861179961,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.04220773659171453
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6023391812865497,
      "acc_stderr": 0.0375363895576169,
      "acc_norm": 0.5906432748538012,
      "acc_norm_stderr": 0.03771283107626545
    }
  },
  "versions": {
    "hendrycksTest-international_law": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "truthfulqa_mc_mt_es": 1,
    "arc_challenge_mt_fr": 0,
    "hendrycksTest-college_medicine": 0,
    "arc_easy": 0,
    "hellaswag_mt_pt": 0,
    "hendrycksTest-nutrition": 0,
    "piqa": 0,
    "hendrycksTest-high_school_geography": 0,
    "hellaswag": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-professional_accounting": 0,
    "arc_challenge_mt_pt": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-astronomy": 0,
    "truthfulqa_mc_mt_nl": 1,
    "hendrycksTest-high_school_microeconomics": 0,
    "hellaswag_mt_it": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-virology": 0,
    "lambada_openai_mt_es": 0,
    "lambada_openai_mt_it": 0,
    "truthfulqa_mc_mt_pt": 1,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest_mt_it": 0,
    "hendrycksTest-college_biology": 0,
    "arc_challenge_mt_es": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest_mt_es": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hellaswag_mt_nl": 0,
    "sciq": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-business_ethics": 0,
    "hellaswag_mt_fr": 0,
    "truthfulqa_mc": 1,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest_mt_fr": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-conceptual_physics": 0,
    "lambada_openai_mt_de": 0,
    "winogrande": 0,
    "truthfulqa_mc_mt_fr": 1,
    "hendrycksTest-high_school_physics": 0,
    "arc_challenge_mt_it": 0,
    "hendrycksTest_mt_nl": 0,
    "hendrycksTest-moral_scenarios": 0,
    "lambada_openai": 0,
    "hellaswag_mt_de": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest_mt_de": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "truthfulqa_mc_mt_it": 1,
    "arc_challenge_mt_de": 0,
    "hendrycksTest_mt_pt": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-high_school_statistics": 0,
    "arc_challenge": 0,
    "arc_challenge_mt_nl": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-human_sexuality": 0,
    "hellaswag_mt_es": 0,
    "lambada_openai_mt_fr": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-formal_logic": 0,
    "truthfulqa_mc_mt_de": 1,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-world_religions": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=microsoft/phi-2,trust_remote_code=True",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}