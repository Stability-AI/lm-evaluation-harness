{
  "results": {
    "hendrycksTest-college_physics": {
      "acc": 0.18627450980392157,
      "acc_stderr": 0.038739587141493524,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.04023382273617746
    },
    "arc_challenge_mt_nl": {
      "acc": 0.2643284858853721,
      "acc_stderr": 0.012903054533598447,
      "acc_norm": 0.2951240376390077,
      "acc_norm_stderr": 0.013345572865502645
    },
    "truthfulqa_mc_mt_de": {
      "mc1": 0.2893401015228426,
      "mc1_stderr": 0.016163963211442943,
      "mc2": 0.47035169832434043,
      "mc2_stderr": 0.01607442782003008
    },
    "hendrycksTest-human_aging": {
      "acc": 0.42152466367713004,
      "acc_stderr": 0.033141902221106564,
      "acc_norm": 0.3452914798206278,
      "acc_norm_stderr": 0.031911001928357954
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "arc_easy": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.009673016668133382,
      "acc_norm": 0.6039562289562289,
      "acc_norm_stderr": 0.010035580962097942
    },
    "truthfulqa_mc_mt_nl": {
      "mc1": 0.3197452229299363,
      "mc1_stderr": 0.016656348583711893,
      "mc2": 0.48281285161953064,
      "mc2_stderr": 0.016220727785671313
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284,
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.51213282247765,
      "acc_stderr": 0.017874698667491338,
      "acc_norm": 0.46360153256704983,
      "acc_norm_stderr": 0.01783252407959326
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.30512820512820515,
      "acc_stderr": 0.023346335293325884,
      "acc_norm": 0.26153846153846155,
      "acc_norm_stderr": 0.02228214120420442
    },
    "hendrycksTest-computer_security": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-management": {
      "acc": 0.4854368932038835,
      "acc_stderr": 0.04948637324026637,
      "acc_norm": 0.47572815533980584,
      "acc_norm_stderr": 0.04944901092973781
    },
    "winogrande": {
      "acc": 0.6408839779005525,
      "acc_stderr": 0.01348311520212023
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3208092485549133,
      "acc_stderr": 0.025131000233647918,
      "acc_norm": 0.3063583815028902,
      "acc_norm_stderr": 0.024818350129436593
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3611111111111111,
      "acc_stderr": 0.04643454608906274,
      "acc_norm": 0.49074074074074076,
      "acc_norm_stderr": 0.04832853553437055
    },
    "hendrycksTest-astronomy": {
      "acc": 0.39473684210526316,
      "acc_stderr": 0.039777499346220734,
      "acc_norm": 0.40789473684210525,
      "acc_norm_stderr": 0.03999309712777471
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.32189542483660133,
      "acc_stderr": 0.018901015322093085,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.01863559403442397
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4580152671755725,
      "acc_stderr": 0.04369802690578756,
      "acc_norm": 0.37404580152671757,
      "acc_norm_stderr": 0.04243869242230524
    },
    "truthfulqa_mc_mt_es": {
      "mc1": 0.2902408111533587,
      "mc1_stderr": 0.016168571172945873,
      "mc2": 0.45402196628837216,
      "mc2_stderr": 0.016211021095355167
    },
    "hendrycksTest_mt_fr": {
      "acc": 0.2978122794636556,
      "acc_stderr": 0.012152518445375725,
      "acc_norm": 0.30275229357798167,
      "acc_norm_stderr": 0.01220971822551059
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.33455882352941174,
      "acc_stderr": 0.028661996202335317,
      "acc_norm": 0.3272058823529412,
      "acc_norm_stderr": 0.028501452860396567
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3191489361702128,
      "acc_stderr": 0.03047297336338004,
      "acc_norm": 0.2127659574468085,
      "acc_norm_stderr": 0.026754391348039773
    },
    "lambada_openai": {
      "ppl": 6.3037575187994355,
      "ppl_stderr": 0.20451865063905555,
      "acc": 0.5920822821657287,
      "acc_stderr": 0.006846827558420366
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.30357142857142855,
      "acc_stderr": 0.043642261558410445,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.043994650575715215
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3619631901840491,
      "acc_stderr": 0.037757007291414416,
      "acc_norm": 0.36809815950920244,
      "acc_norm_stderr": 0.03789213935838396
    },
    "hendrycksTest_mt_pt": {
      "acc": 0.30074677528852684,
      "acc_stderr": 0.01195263957005443,
      "acc_norm": 0.3170400543109301,
      "acc_norm_stderr": 0.012128323678073368
    },
    "hellaswag_mt_de": {
      "acc": 0.3836464560204953,
      "acc_stderr": 0.005024354615062105,
      "acc_norm": 0.46637489325362935,
      "acc_norm_stderr": 0.005154486998966453
    },
    "hendrycksTest-prehistory": {
      "acc": 0.36419753086419754,
      "acc_stderr": 0.026774929899722334,
      "acc_norm": 0.2839506172839506,
      "acc_norm_stderr": 0.025089478523765127
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.46422018348623856,
      "acc_stderr": 0.02138236477570191,
      "acc_norm": 0.3504587155963303,
      "acc_norm_stderr": 0.020456077599824457
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "truthfulqa_mc_mt_pt": {
      "mc1": 0.30710659898477155,
      "mc1_stderr": 0.016443354533552747,
      "mc2": 0.45063623144660114,
      "mc2_stderr": 0.016129350768468827
    },
    "hendrycksTest-public_relations": {
      "acc": 0.37272727272727274,
      "acc_stderr": 0.04631381319425463,
      "acc_norm": 0.24545454545454545,
      "acc_norm_stderr": 0.041220665028782834
    },
    "lambada_openai_mt_es": {
      "ppl": 396.3801983371395,
      "ppl_stderr": 28.98414089536596,
      "acc": 0.18455268775470599,
      "acc_stderr": 0.005404682831182018
    },
    "lambada_openai_mt_fr": {
      "ppl": 101.31738066534355,
      "ppl_stderr": 7.324679057678024,
      "acc": 0.37162817776052787,
      "acc_stderr": 0.006732474880984059
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3319327731092437,
      "acc_stderr": 0.03058869701378366,
      "acc_norm": 0.37815126050420167,
      "acc_norm_stderr": 0.031499305777849054
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "piqa": {
      "acc": 0.749727965179543,
      "acc_stderr": 0.010106561880089798,
      "acc_norm": 0.7535364526659413,
      "acc_norm_stderr": 0.01005481078967181
    },
    "hendrycksTest-security_studies": {
      "acc": 0.4122448979591837,
      "acc_stderr": 0.03151236044674281,
      "acc_norm": 0.3183673469387755,
      "acc_norm_stderr": 0.029822533793982073
    },
    "hendrycksTest-international_law": {
      "acc": 0.32231404958677684,
      "acc_stderr": 0.042664163633521664,
      "acc_norm": 0.4793388429752066,
      "acc_norm_stderr": 0.045604560863872344
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.39215686274509803,
      "acc_stderr": 0.03426712349247271,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.03228210387037894
    },
    "hendrycksTest_mt_es": {
      "acc": 0.3180887372013652,
      "acc_stderr": 0.012172156832638064,
      "acc_norm": 0.3126279863481229,
      "acc_norm_stderr": 0.012115443349488775
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2797427652733119,
      "acc_stderr": 0.02549425935069489,
      "acc_norm": 0.3247588424437299,
      "acc_norm_stderr": 0.02659678228769705
    },
    "hendrycksTest-anatomy": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.04171654161354543,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.04171654161354543
    },
    "truthfulqa_mc_mt_fr": {
      "mc1": 0.3062261753494282,
      "mc1_stderr": 0.016440644449778455,
      "mc2": 0.4653355775725746,
      "mc2_stderr": 0.016073878652616367
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.41968911917098445,
      "acc_stderr": 0.03561587327685884,
      "acc_norm": 0.35751295336787564,
      "acc_norm_stderr": 0.03458816042181005
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "arc_challenge_mt_fr": {
      "acc": 0.30025662959794697,
      "acc_stderr": 0.013412022629366676,
      "acc_norm": 0.32164242942686055,
      "acc_norm_stderr": 0.013667673121050187
    },
    "hendrycksTest-econometrics": {
      "acc": 0.30701754385964913,
      "acc_stderr": 0.04339138322579861,
      "acc_norm": 0.30701754385964913,
      "acc_norm_stderr": 0.04339138322579861
    },
    "arc_challenge": {
      "acc": 0.3643344709897611,
      "acc_stderr": 0.014063260279882417,
      "acc_norm": 0.3916382252559727,
      "acc_norm_stderr": 0.014264122124938217
    },
    "hendrycksTest-virology": {
      "acc": 0.4578313253012048,
      "acc_stderr": 0.038786267710023595,
      "acc_norm": 0.35542168674698793,
      "acc_norm_stderr": 0.03726214354322416
    },
    "arc_challenge_mt_de": {
      "acc": 0.2686056458511548,
      "acc_stderr": 0.012969163006892298,
      "acc_norm": 0.3045337895637297,
      "acc_norm_stderr": 0.013465867574563839
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2894393741851369,
      "acc_stderr": 0.01158265970221025,
      "acc_norm": 0.3011734028683181,
      "acc_norm_stderr": 0.01171714875164842
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.26108374384236455,
      "acc_stderr": 0.03090379695211449,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.03161856335358611
    },
    "arc_challenge_mt_es": {
      "acc": 0.30512820512820515,
      "acc_stderr": 0.013467477651314567,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.013787530543184118
    },
    "lambada_openai_mt_de": {
      "ppl": 190.1358519771474,
      "ppl_stderr": 14.675054244172427,
      "acc": 0.29749660392004657,
      "acc_stderr": 0.006369088639380678
    },
    "hellaswag_mt_nl": {
      "acc": 0.38866702644360496,
      "acc_stderr": 0.005064405668097518,
      "acc_norm": 0.47987048030221263,
      "acc_norm_stderr": 0.00519061115204816
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.24822695035460993,
      "acc_stderr": 0.025770015644290396,
      "acc_norm": 0.2375886524822695,
      "acc_norm_stderr": 0.025389512552729903
    },
    "sciq": {
      "acc": 0.887,
      "acc_stderr": 0.010016552866696848,
      "acc_norm": 0.776,
      "acc_norm_stderr": 0.013190830072364457
    },
    "hendrycksTest_mt_de": {
      "acc": 0.3140096618357488,
      "acc_stderr": 0.012196809034667177,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.012060860889940168
    },
    "hellaswag_mt_pt": {
      "acc": 0.4000433416404811,
      "acc_stderr": 0.005099876614960208,
      "acc_norm": 0.5121898363853071,
      "acc_norm_stderr": 0.005203398741415707
    },
    "hendrycksTest-marketing": {
      "acc": 0.5641025641025641,
      "acc_stderr": 0.032485775115784016,
      "acc_norm": 0.5170940170940171,
      "acc_norm_stderr": 0.032736940493481824
    },
    "hendrycksTest-world_religions": {
      "acc": 0.49707602339181284,
      "acc_stderr": 0.03834759370936839,
      "acc_norm": 0.5029239766081871,
      "acc_norm_stderr": 0.03834759370936839
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.027634907264178544,
      "acc_norm": 0.32222222222222224,
      "acc_norm_stderr": 0.028493465091028597
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3870967741935484,
      "acc_stderr": 0.02770935967503249,
      "acc_norm": 0.3387096774193548,
      "acc_norm_stderr": 0.02692344605930284
    },
    "hendrycksTest_mt_nl": {
      "acc": 0.3258897418004187,
      "acc_stderr": 0.012385949475361174,
      "acc_norm": 0.31123517096999304,
      "acc_norm_stderr": 0.012235122059731571
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.42616033755274263,
      "acc_stderr": 0.03219035703131774,
      "acc_norm": 0.4050632911392405,
      "acc_norm_stderr": 0.031955147413706725
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.01433352205921789
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hellaswag_mt_it": {
      "acc": 0.39247253344936367,
      "acc_stderr": 0.005093104477827312,
      "acc_norm": 0.48982921788317196,
      "acc_norm_stderr": 0.005214049224113798
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.43636363636363634,
      "acc_stderr": 0.03872592983524754,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.0380491365397101
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2119205298013245,
      "acc_stderr": 0.033367670865679766,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.03684881521389024
    },
    "hendrycksTest_mt_it": {
      "acc": 0.314404432132964,
      "acc_stderr": 0.01222208120635071,
      "acc_norm": 0.3296398891966759,
      "acc_norm_stderr": 0.012374874300854282
    },
    "hellaswag": {
      "acc": 0.5324636526588329,
      "acc_stderr": 0.00497925295497732,
      "acc_norm": 0.6880103565026887,
      "acc_norm_stderr": 0.004623587630639281
    },
    "arc_challenge_mt_pt": {
      "acc": 0.30085470085470084,
      "acc_stderr": 0.013413893880618116,
      "acc_norm": 0.33931623931623933,
      "acc_norm_stderr": 0.013848153952086445
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.335978835978836,
      "acc_stderr": 0.024326310529149138,
      "acc_norm": 0.31746031746031744,
      "acc_norm_stderr": 0.023973861998992072
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4494949494949495,
      "acc_stderr": 0.035441324919479704,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.035402943770953675
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.03010279378179119,
      "acc_norm": 0.36981132075471695,
      "acc_norm_stderr": 0.02971142188010793
    },
    "truthfulqa_mc_mt_it": {
      "mc1": 0.3218390804597701,
      "mc1_stderr": 0.01670638141505791,
      "mc2": 0.47540286358317524,
      "mc2_stderr": 0.016562304332331048
    },
    "hellaswag_mt_es": {
      "acc": 0.4182846170258161,
      "acc_stderr": 0.005095090615690128,
      "acc_norm": 0.5196287604011095,
      "acc_norm_stderr": 0.005160547508589296
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.04006168083848876,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488774
    },
    "hellaswag_mt_fr": {
      "acc": 0.4089740843863782,
      "acc_stderr": 0.00508800438842195,
      "acc_norm": 0.5092096808738488,
      "acc_norm_stderr": 0.005173597526002336
    },
    "lambada_openai_mt_it": {
      "ppl": 137.12205143688394,
      "ppl_stderr": 10.790390751752167,
      "acc": 0.35610324083058414,
      "acc_stderr": 0.006671264434701404
    },
    "arc_challenge_mt_it": {
      "acc": 0.2694610778443114,
      "acc_stderr": 0.012982199528535547,
      "acc_norm": 0.31223267750213857,
      "acc_norm_stderr": 0.013559339165224312
    },
    "truthfulqa_mc": {
      "mc1": 0.30599755201958384,
      "mc1_stderr": 0.016132229728155048,
      "mc2": 0.4505740037385471,
      "mc2_stderr": 0.01575743626887178
    },
    "hendrycksTest-sociology": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03333333333333333,
      "acc_norm": 0.34328358208955223,
      "acc_norm_stderr": 0.03357379665433431
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.028074158947600666,
      "acc_norm": 0.43790849673202614,
      "acc_norm_stderr": 0.02840830202033269
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3125,
      "acc_stderr": 0.038760854559127644,
      "acc_norm": 0.3472222222222222,
      "acc_norm_stderr": 0.039812405437178615
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25462962962962965,
      "acc_stderr": 0.029711275860005344,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.031415546294025445
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3063583815028902,
      "acc_stderr": 0.03514942551267438,
      "acc_norm": 0.24277456647398843,
      "acc_norm_stderr": 0.0326926380614177
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.36551724137931035,
      "acc_stderr": 0.04013124195424385,
      "acc_norm": 0.3586206896551724,
      "acc_norm_stderr": 0.03996629574876719
    }
  },
  "versions": {
    "hendrycksTest-college_physics": 0,
    "arc_challenge_mt_nl": 0,
    "truthfulqa_mc_mt_de": 1,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-college_computer_science": 0,
    "arc_easy": 0,
    "truthfulqa_mc_mt_nl": 1,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-management": 0,
    "winogrande": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-human_sexuality": 0,
    "truthfulqa_mc_mt_es": 1,
    "hendrycksTest_mt_fr": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-conceptual_physics": 0,
    "lambada_openai": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest_mt_pt": 0,
    "hellaswag_mt_de": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "truthfulqa_mc_mt_pt": 1,
    "hendrycksTest-public_relations": 0,
    "lambada_openai_mt_es": 0,
    "lambada_openai_mt_fr": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-abstract_algebra": 0,
    "piqa": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest_mt_es": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-anatomy": 0,
    "truthfulqa_mc_mt_fr": 1,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-college_chemistry": 0,
    "arc_challenge_mt_fr": 0,
    "hendrycksTest-econometrics": 0,
    "arc_challenge": 0,
    "hendrycksTest-virology": 0,
    "arc_challenge_mt_de": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "arc_challenge_mt_es": 0,
    "lambada_openai_mt_de": 0,
    "hellaswag_mt_nl": 0,
    "hendrycksTest-professional_accounting": 0,
    "sciq": 0,
    "hendrycksTest_mt_de": 0,
    "hellaswag_mt_pt": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-world_religions": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest_mt_nl": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hellaswag_mt_it": 0,
    "hendrycksTest-high_school_european_history": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest_mt_it": 0,
    "hellaswag": 0,
    "arc_challenge_mt_pt": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "truthfulqa_mc_mt_it": 1,
    "hellaswag_mt_es": 0,
    "hendrycksTest-formal_logic": 0,
    "hellaswag_mt_fr": 0,
    "lambada_openai_mt_it": 0,
    "arc_challenge_mt_it": 0,
    "truthfulqa_mc": 1,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-electrical_engineering": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=stabilityai/stablelm-2-zephyr-1_6b,dtype=auto,trust_remote_code=True,use_fast=False",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}