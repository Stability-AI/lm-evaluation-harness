{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036845,
      "acc_norm": 0.19,
      "acc_norm_stderr": 0.039427724440366234
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421255,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "lambada_openai_mt_fr": {
      "ppl": 47.01010923787393,
      "ppl_stderr": 2.5481935089342183,
      "acc": 0.4135455074713759,
      "acc_stderr": 0.006861054754557159
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3319327731092437,
      "acc_stderr": 0.030588697013783663,
      "acc_norm": 0.36134453781512604,
      "acc_norm_stderr": 0.03120469122515001
    },
    "hendrycksTest_mt_nl": {
      "acc": 0.29239357990230286,
      "acc_stderr": 0.012020105780350787,
      "acc_norm": 0.2958827634333566,
      "acc_norm_stderr": 0.012061763589018597
    },
    "truthfulqa_mc_mt_it": {
      "mc1": 0.28991060025542786,
      "mc1_stderr": 0.016225017944770968,
      "mc2": 0.4396834411933492,
      "mc2_stderr": 0.015327080381906198
    },
    "truthfulqa_mc_mt_de": {
      "mc1": 0.2715736040609137,
      "mc1_stderr": 0.01585437948860159,
      "mc2": 0.43786575187377635,
      "mc2_stderr": 0.015122310295922972
    },
    "truthfulqa_mc_mt_pt": {
      "mc1": 0.2753807106598985,
      "mc1_stderr": 0.015923346195889237,
      "mc2": 0.4228433322120437,
      "mc2_stderr": 0.015033914053820539
    },
    "lambada_openai_mt_es": {
      "ppl": 138.6873803320386,
      "ppl_stderr": 7.5814030378427715,
      "acc": 0.2293809431399185,
      "acc_stderr": 0.005857477272420425
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.27167630057803466,
      "acc_stderr": 0.03391750322321659,
      "acc_norm": 0.2832369942196532,
      "acc_norm_stderr": 0.03435568056047874
    },
    "hendrycksTest-astronomy": {
      "acc": 0.34868421052631576,
      "acc_stderr": 0.038781398887976104,
      "acc_norm": 0.3815789473684211,
      "acc_norm_stderr": 0.039531733777491945
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-marketing": {
      "acc": 0.5213675213675214,
      "acc_stderr": 0.032726164476349545,
      "acc_norm": 0.47863247863247865,
      "acc_norm_stderr": 0.032726164476349545
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2936170212765957,
      "acc_stderr": 0.029771642712491227,
      "acc_norm": 0.2297872340425532,
      "acc_norm_stderr": 0.027501752944412417
    },
    "truthfulqa_mc": {
      "mc1": 0.24724602203182375,
      "mc1_stderr": 0.01510240479735965,
      "mc2": 0.38771474171764936,
      "mc2_stderr": 0.014006161260636432
    },
    "hendrycksTest-computer_security": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.30514705882352944,
      "acc_stderr": 0.027971541370170598,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.02815637344037142
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-international_law": {
      "acc": 0.256198347107438,
      "acc_stderr": 0.03984979653302872,
      "acc_norm": 0.5206611570247934,
      "acc_norm_stderr": 0.04560456086387235
    },
    "hendrycksTest_mt_it": {
      "acc": 0.28462603878116344,
      "acc_stderr": 0.011878749197079416,
      "acc_norm": 0.29293628808864264,
      "acc_norm_stderr": 0.011980713862273996
    },
    "arc_challenge_mt_fr": {
      "acc": 0.2634730538922156,
      "acc_stderr": 0.012889646336321777,
      "acc_norm": 0.3088109495295124,
      "acc_norm_stderr": 0.013518339489431923
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.031447125816782405,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.03161856335358611
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2847222222222222,
      "acc_stderr": 0.037738099906869355,
      "acc_norm": 0.2847222222222222,
      "acc_norm_stderr": 0.03773809990686935
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.04134913018303316,
      "acc_norm": 0.31746031746031744,
      "acc_norm_stderr": 0.04163453031302859
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.02791405551046801,
      "acc_norm": 0.4215686274509804,
      "acc_norm_stderr": 0.028275490156791434
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.24822695035460993,
      "acc_stderr": 0.025770015644290396,
      "acc_norm": 0.25177304964539005,
      "acc_norm_stderr": 0.0258921511567094
    },
    "hendrycksTest_mt_es": {
      "acc": 0.2901023890784983,
      "acc_stderr": 0.011860499283665137,
      "acc_norm": 0.27918088737201363,
      "acc_norm_stderr": 0.01172426024727164
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.3626943005181347,
      "acc_stderr": 0.034697137917043715,
      "acc_norm": 0.3160621761658031,
      "acc_norm_stderr": 0.033553973696861736
    },
    "arc_easy": {
      "acc": 0.686026936026936,
      "acc_stderr": 0.00952324533521551,
      "acc_norm": 0.6818181818181818,
      "acc_norm_stderr": 0.009557408782506376
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4776500638569604,
      "acc_stderr": 0.01786209177850787,
      "acc_norm": 0.40229885057471265,
      "acc_norm_stderr": 0.017535294529068962
    },
    "arc_challenge_mt_pt": {
      "acc": 0.2743589743589744,
      "acc_stderr": 0.01305008210311032,
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.013435492568854207
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25,
      "acc_stderr": 0.029531221160930918,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.030851992993257013
    },
    "hellaswag_mt_es": {
      "acc": 0.40228291017708556,
      "acc_stderr": 0.005064940171749539,
      "acc_norm": 0.5181352677618946,
      "acc_norm_stderr": 0.005161130487005257
    },
    "hendrycksTest-virology": {
      "acc": 0.35542168674698793,
      "acc_stderr": 0.03726214354322415,
      "acc_norm": 0.3253012048192771,
      "acc_norm_stderr": 0.03647168523683227
    },
    "hendrycksTest-college_physics": {
      "acc": 0.17647058823529413,
      "acc_stderr": 0.03793281185307809,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.042801058373643945
    },
    "hendrycksTest-prehistory": {
      "acc": 0.33024691358024694,
      "acc_stderr": 0.02616829845673284,
      "acc_norm": 0.2932098765432099,
      "acc_norm_stderr": 0.02532988817190092
    },
    "sciq": {
      "acc": 0.955,
      "acc_stderr": 0.006558812241406125,
      "acc_norm": 0.949,
      "acc_norm_stderr": 0.006960420062571413
    },
    "truthfulqa_mc_mt_nl": {
      "mc1": 0.27770700636942675,
      "mc1_stderr": 0.015995288886407878,
      "mc2": 0.44466396664068425,
      "mc2_stderr": 0.015214536135740345
    },
    "truthfulqa_mc_mt_fr": {
      "mc1": 0.2770012706480305,
      "mc1_stderr": 0.015962406802404735,
      "mc2": 0.4370276482917051,
      "mc2_stderr": 0.014888462809341127
    },
    "hendrycksTest_mt_de": {
      "acc": 0.26017943409247757,
      "acc_stderr": 0.011529630820464843,
      "acc_norm": 0.2781228433402346,
      "acc_norm_stderr": 0.01177512968552351
    },
    "arc_challenge_mt_nl": {
      "acc": 0.2523524379811805,
      "acc_stderr": 0.012709568078481333,
      "acc_norm": 0.2865697177074423,
      "acc_norm_stderr": 0.013230294037120463
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3212121212121212,
      "acc_stderr": 0.03646204963253811,
      "acc_norm": 0.3393939393939394,
      "acc_norm_stderr": 0.036974422050315967
    },
    "lambada_openai_mt_de": {
      "ppl": 71.30252459096387,
      "ppl_stderr": 4.129606514952369,
      "acc": 0.34445953813312635,
      "acc_stderr": 0.0066203497203462785
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3877551020408163,
      "acc_stderr": 0.03119223072679566,
      "acc_norm": 0.27755102040816326,
      "acc_norm_stderr": 0.028666857790274645
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4619883040935672,
      "acc_stderr": 0.03823727092882307,
      "acc_norm": 0.4678362573099415,
      "acc_norm_stderr": 0.038268824176603704
    },
    "hellaswag": {
      "acc": 0.5167297351125274,
      "acc_stderr": 0.004986987508928693,
      "acc_norm": 0.6896036646086438,
      "acc_norm_stderr": 0.004617103280372017
    },
    "hellaswag_mt_it": {
      "acc": 0.3809420211030132,
      "acc_stderr": 0.005065124161781253,
      "acc_norm": 0.48689220058740346,
      "acc_norm_stderr": 0.005213335915305624
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-sociology": {
      "acc": 0.3283582089552239,
      "acc_stderr": 0.033206858897443244,
      "acc_norm": 0.2835820895522388,
      "acc_norm_stderr": 0.031871875379197986
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3851851851851852,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.34074074074074073,
      "acc_norm_stderr": 0.04094376269996794
    },
    "winogrande": {
      "acc": 0.6353591160220995,
      "acc_stderr": 0.013527746622429844
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.33815028901734107,
      "acc_stderr": 0.025469770149400172,
      "acc_norm": 0.3179190751445087,
      "acc_norm_stderr": 0.025070713719153172
    },
    "hellaswag_mt_pt": {
      "acc": 0.3898580561274244,
      "acc_stderr": 0.005077090493188585,
      "acc_norm": 0.5096976920576444,
      "acc_norm_stderr": 0.005203966706349455
    },
    "hendrycksTest-econometrics": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.04372748290278007,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322004
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.29464285714285715,
      "acc_stderr": 0.04327040932578728,
      "acc_norm": 0.22321428571428573,
      "acc_norm_stderr": 0.039523019677025116
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3283018867924528,
      "acc_stderr": 0.02890159361241178,
      "acc_norm": 0.38113207547169814,
      "acc_norm_stderr": 0.029890609686286627
    },
    "arc_challenge": {
      "acc": 0.34812286689419797,
      "acc_stderr": 0.013921008595179342,
      "acc_norm": 0.3890784982935154,
      "acc_norm_stderr": 0.014247309976045609
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.40458015267175573,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.3435114503816794,
      "acc_norm_stderr": 0.041649760719448786
    },
    "hendrycksTest-management": {
      "acc": 0.42718446601941745,
      "acc_stderr": 0.04897957737781168,
      "acc_norm": 0.4077669902912621,
      "acc_norm_stderr": 0.048657775704107696
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.0471282125742677,
      "acc_norm": 0.49074074074074076,
      "acc_norm_stderr": 0.04832853553437055
    },
    "hellaswag_mt_fr": {
      "acc": 0.38894838295138146,
      "acc_stderr": 0.005045233312181468,
      "acc_norm": 0.5035339473120582,
      "acc_norm_stderr": 0.005174346132646826
    },
    "arc_challenge_mt_es": {
      "acc": 0.2675213675213675,
      "acc_stderr": 0.012947009372942183,
      "acc_norm": 0.30683760683760686,
      "acc_norm_stderr": 0.013488527262924295
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.31794871794871793,
      "acc_stderr": 0.02361088430892786,
      "acc_norm": 0.29743589743589743,
      "acc_norm_stderr": 0.023177408131465932
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4696969696969697,
      "acc_stderr": 0.03555804051763929,
      "acc_norm": 0.4292929292929293,
      "acc_norm_stderr": 0.035265527246011986
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.4036697247706422,
      "acc_stderr": 0.021035704856574963,
      "acc_norm": 0.3339449541284404,
      "acc_norm_stderr": 0.020220554196736407
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.23178807947019867,
      "acc_stderr": 0.034454062719870546,
      "acc_norm": 0.2582781456953642,
      "acc_norm_stderr": 0.035737053147634576
    },
    "arc_challenge_mt_it": {
      "acc": 0.24294268605645852,
      "acc_stderr": 0.012548588352773895,
      "acc_norm": 0.28999144568006846,
      "acc_norm_stderr": 0.013277091943380967
    },
    "piqa": {
      "acc": 0.7627856365614799,
      "acc_stderr": 0.009924694933586373,
      "acc_norm": 0.766050054406964,
      "acc_norm_stderr": 0.009877236895137434
    },
    "hendrycksTest-public_relations": {
      "acc": 0.41818181818181815,
      "acc_stderr": 0.04724577405731571,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "hendrycksTest-human_aging": {
      "acc": 0.35874439461883406,
      "acc_stderr": 0.03219079200419997,
      "acc_norm": 0.27802690582959644,
      "acc_norm_stderr": 0.030069584874494033
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.047609522856952365
    },
    "hellaswag_mt_nl": {
      "acc": 0.37679438747976257,
      "acc_stderr": 0.005034642265132649,
      "acc_norm": 0.48332433890987586,
      "acc_norm_stderr": 0.0051919327859975065
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3128834355828221,
      "acc_stderr": 0.03642914578292406,
      "acc_norm": 0.3312883435582822,
      "acc_norm_stderr": 0.03697983910025588
    },
    "lambada_openai": {
      "ppl": 4.888717298517636,
      "ppl_stderr": 0.11244908017508667,
      "acc": 0.6613623132156026,
      "acc_stderr": 0.006593248415229686
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909284,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3235294117647059,
      "acc_stderr": 0.03283472056108566,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.03198001660115072
    },
    "hellaswag_mt_de": {
      "acc": 0.37073014517506403,
      "acc_stderr": 0.004990535556507314,
      "acc_norm": 0.4579419299743809,
      "acc_norm_stderr": 0.005147873332235909
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3931034482758621,
      "acc_stderr": 0.040703290137070705,
      "acc_norm": 0.3793103448275862,
      "acc_norm_stderr": 0.04043461861916747
    },
    "hendrycksTest-global_facts": {
      "acc": 0.24,
      "acc_stderr": 0.042923469599092816,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-professional_law": {
      "acc": 0.26988265971316816,
      "acc_stderr": 0.011337381084250404,
      "acc_norm": 0.2940026075619296,
      "acc_norm_stderr": 0.011636062953698609
    },
    "hendrycksTest_mt_fr": {
      "acc": 0.27593507410021173,
      "acc_stderr": 0.011878472105587625,
      "acc_norm": 0.29075511644318985,
      "acc_norm_stderr": 0.012067857185588721
    },
    "arc_challenge_mt_de": {
      "acc": 0.2292557741659538,
      "acc_stderr": 0.012299681341843569,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.01321845018210016
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3755274261603376,
      "acc_stderr": 0.03152256243091156,
      "acc_norm": 0.350210970464135,
      "acc_norm_stderr": 0.031052391937584353
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.02504044387700068,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.02696242432507383
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3366013071895425,
      "acc_stderr": 0.019117213911495158,
      "acc_norm": 0.27124183006535946,
      "acc_norm_stderr": 0.0179866153040303
    },
    "lambada_openai_mt_it": {
      "ppl": 60.087871346994696,
      "ppl_stderr": 3.5491256823006117,
      "acc": 0.4017077430622938,
      "acc_stderr": 0.0068300497243013875
    },
    "hendrycksTest_mt_pt": {
      "acc": 0.27970128988458925,
      "acc_stderr": 0.011699024436795987,
      "acc_norm": 0.2987101154107264,
      "acc_norm_stderr": 0.011929434230556157
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "truthfulqa_mc_mt_es": {
      "mc1": 0.2534854245880862,
      "mc1_stderr": 0.015496461201952133,
      "mc2": 0.4174691636537592,
      "mc2_stderr": 0.015319224389846824
    },
    "hendrycksTest-philosophy": {
      "acc": 0.33762057877813506,
      "acc_stderr": 0.026858825879488554,
      "acc_norm": 0.33440514469453375,
      "acc_norm_stderr": 0.02679542232789395
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30687830687830686,
      "acc_stderr": 0.02375292871211214,
      "acc_norm": 0.2830687830687831,
      "acc_norm_stderr": 0.023201392938194978
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3387096774193548,
      "acc_stderr": 0.026923446059302844,
      "acc_norm": 0.3387096774193548,
      "acc_norm_stderr": 0.02692344605930284
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-college_computer_science": 0,
    "lambada_openai_mt_fr": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest_mt_nl": 0,
    "truthfulqa_mc_mt_it": 1,
    "truthfulqa_mc_mt_de": 1,
    "truthfulqa_mc_mt_pt": 1,
    "lambada_openai_mt_es": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-conceptual_physics": 0,
    "truthfulqa_mc": 1,
    "hendrycksTest-computer_security": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest_mt_it": 0,
    "arc_challenge_mt_fr": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest_mt_es": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "arc_easy": 0,
    "hendrycksTest-miscellaneous": 0,
    "arc_challenge_mt_pt": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hellaswag_mt_es": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-prehistory": 0,
    "sciq": 0,
    "truthfulqa_mc_mt_nl": 1,
    "truthfulqa_mc_mt_fr": 1,
    "hendrycksTest_mt_de": 0,
    "arc_challenge_mt_nl": 0,
    "hendrycksTest-high_school_european_history": 0,
    "lambada_openai_mt_de": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-world_religions": 0,
    "hellaswag": 0,
    "hellaswag_mt_it": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-anatomy": 0,
    "winogrande": 0,
    "hendrycksTest-moral_disputes": 0,
    "hellaswag_mt_pt": 0,
    "hendrycksTest-econometrics": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "arc_challenge": 0,
    "hendrycksTest-human_sexuality": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-jurisprudence": 0,
    "hellaswag_mt_fr": 0,
    "arc_challenge_mt_es": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_physics": 0,
    "arc_challenge_mt_it": 0,
    "piqa": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hellaswag_mt_nl": 0,
    "hendrycksTest-logical_fallacies": 0,
    "lambada_openai": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hellaswag_mt_de": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest_mt_fr": 0,
    "arc_challenge_mt_de": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "hendrycksTest-professional_psychology": 0,
    "lambada_openai_mt_it": 0,
    "hendrycksTest_mt_pt": 0,
    "hendrycksTest-business_ethics": 0,
    "truthfulqa_mc_mt_es": 1,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-high_school_biology": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=stabilityai/stablelm-2-1_6b,trust_remote_code=True",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}