{
  "results": {
    "hendrycksTest-college_physics": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.04533838195929775,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.04655010411319619
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.02865749128507199,
      "acc_norm": 0.33613445378151263,
      "acc_norm_stderr": 0.030684737115135363
    },
    "sciq": {
      "acc": 0.888,
      "acc_stderr": 0.009977753031397241,
      "acc_norm": 0.818,
      "acc_norm_stderr": 0.012207580637662155
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.24352331606217617,
      "acc_stderr": 0.030975436386845426,
      "acc_norm": 0.23834196891191708,
      "acc_norm_stderr": 0.030748905363909895
    },
    "truthfulqa_mc_mt_it": {
      "mc1": 0.26947637292464877,
      "mc1_stderr": 0.015866243073215047,
      "mc2": 0.41952757831066434,
      "mc2_stderr": 0.01531964843503135
    },
    "hellaswag_mt_pt": {
      "acc": 0.32213674287571786,
      "acc_stderr": 0.00486449079271623,
      "acc_norm": 0.4024271318669412,
      "acc_norm_stderr": 0.005104876844652959
    },
    "lambada_openai_mt_it": {
      "ppl": 137.29154580867956,
      "ppl_stderr": 8.396031093879856,
      "acc": 0.32272462643120514,
      "acc_stderr": 0.006513445272507077
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2023121387283237,
      "acc_stderr": 0.030631145539198823,
      "acc_norm": 0.24277456647398843,
      "acc_norm_stderr": 0.0326926380614177
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.25251396648044694,
      "acc_stderr": 0.014530330201468643,
      "acc_norm": 0.27262569832402234,
      "acc_norm_stderr": 0.014893391735249588
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.270042194092827,
      "acc_stderr": 0.028900721906293426,
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "truthfulqa_mc_mt_pt": {
      "mc1": 0.26776649746192893,
      "mc1_stderr": 0.015783944695917682,
      "mc2": 0.42257893028568183,
      "mc2_stderr": 0.015426042893154674
    },
    "arc_easy": {
      "acc": 0.6031144781144782,
      "acc_stderr": 0.010039236800583206,
      "acc_norm": 0.5534511784511784,
      "acc_norm_stderr": 0.0102009900762453
    },
    "winogrande": {
      "acc": 0.5911602209944752,
      "acc_stderr": 0.0138169542951357
    },
    "hendrycksTest-prehistory": {
      "acc": 0.2654320987654321,
      "acc_stderr": 0.024569223600460856,
      "acc_norm": 0.19753086419753085,
      "acc_norm_stderr": 0.02215288992789894
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.25165562913907286,
      "acc_stderr": 0.035433042343899844,
      "acc_norm": 0.2781456953642384,
      "acc_norm_stderr": 0.03658603262763743
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2512820512820513,
      "acc_stderr": 0.021992016662370547,
      "acc_norm": 0.2641025641025641,
      "acc_norm_stderr": 0.022352193737453282
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3435114503816794,
      "acc_stderr": 0.041649760719448786,
      "acc_norm": 0.2824427480916031,
      "acc_norm_stderr": 0.03948406125768361
    },
    "arc_challenge_mt_nl": {
      "acc": 0.1924721984602224,
      "acc_stderr": 0.011535630155404803,
      "acc_norm": 0.22840034217279725,
      "acc_norm_stderr": 0.012283523666042804
    },
    "truthfulqa_mc_mt_de": {
      "mc1": 0.25,
      "mc1_stderr": 0.015435235849118604,
      "mc2": 0.4249483809341532,
      "mc2_stderr": 0.015332750179403701
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322004,
      "acc_norm": 0.2719298245614035,
      "acc_norm_stderr": 0.04185774424022056
    },
    "hellaswag": {
      "acc": 0.44981079466241786,
      "acc_stderr": 0.00496457968571244,
      "acc_norm": 0.5920135431189006,
      "acc_norm_stderr": 0.004904561795919018
    },
    "hendrycksTest-sociology": {
      "acc": 0.31840796019900497,
      "acc_stderr": 0.03294118479054095,
      "acc_norm": 0.30845771144278605,
      "acc_norm_stderr": 0.032658195885126994
    },
    "arc_challenge_mt_fr": {
      "acc": 0.2369546621043627,
      "acc_stderr": 0.012441890624187796,
      "acc_norm": 0.2737382378100941,
      "acc_norm_stderr": 0.013046466448429678
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2698412698412698,
      "acc_stderr": 0.03970158273235173,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.038522733649243156
    },
    "hellaswag_mt_de": {
      "acc": 0.3123398804440649,
      "acc_stderr": 0.004788509782226098,
      "acc_norm": 0.3619769427839453,
      "acc_norm_stderr": 0.004965447342212407
    },
    "hendrycksTest_mt_it": {
      "acc": 0.24307479224376732,
      "acc_stderr": 0.011291802680487486,
      "acc_norm": 0.25415512465373963,
      "acc_norm_stderr": 0.011461475357507167
    },
    "hellaswag_mt_nl": {
      "acc": 0.3042633567188343,
      "acc_stderr": 0.004780221178573578,
      "acc_norm": 0.3590933621154884,
      "acc_norm_stderr": 0.0049842727545814005
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.23669724770642203,
      "acc_stderr": 0.018224078117299067,
      "acc_norm": 0.23853211009174313,
      "acc_norm_stderr": 0.018272575810231863
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.23773584905660378,
      "acc_stderr": 0.026199808807561915,
      "acc_norm": 0.33962264150943394,
      "acc_norm_stderr": 0.029146904747798335
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.30357142857142855,
      "acc_stderr": 0.04364226155841044,
      "acc_norm": 0.21428571428571427,
      "acc_norm_stderr": 0.038946411200447915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2973856209150327,
      "acc_stderr": 0.026173908506718576,
      "acc_norm": 0.35947712418300654,
      "acc_norm_stderr": 0.027475969910660952
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03173071239071724,
      "acc_norm": 0.2878787878787879,
      "acc_norm_stderr": 0.03225883512300993
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.031321798030832904,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.03242661719827218
    },
    "lambada_openai": {
      "ppl": 6.92882187979878,
      "ppl_stderr": 0.17702371584447624,
      "acc": 0.5882010479332428,
      "acc_stderr": 0.006856738524093183
    },
    "arc_challenge_mt_es": {
      "acc": 0.23333333333333334,
      "acc_stderr": 0.01237041950686314,
      "acc_norm": 0.2658119658119658,
      "acc_norm_stderr": 0.01292062907954085
    },
    "hellaswag_mt_es": {
      "acc": 0.33550245359505015,
      "acc_stderr": 0.00487702781650109,
      "acc_norm": 0.41423085129080434,
      "acc_norm_stderr": 0.005087977234271633
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-professional_law": {
      "acc": 0.242503259452412,
      "acc_stderr": 0.010946570966348775,
      "acc_norm": 0.2685788787483703,
      "acc_norm_stderr": 0.011320056629121725
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.022418042891113953,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.022569897074918428
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.04524596007030048,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.047128212574267705
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2369281045751634,
      "acc_stderr": 0.01720166216978978,
      "acc_norm": 0.25326797385620914,
      "acc_norm_stderr": 0.01759348689536683
    },
    "hendrycksTest-international_law": {
      "acc": 0.2644628099173554,
      "acc_stderr": 0.04026187527591204,
      "acc_norm": 0.4628099173553719,
      "acc_norm_stderr": 0.04551711196104218
    },
    "hendrycksTest-astronomy": {
      "acc": 0.23026315789473684,
      "acc_stderr": 0.03426059424403165,
      "acc_norm": 0.34868421052631576,
      "acc_norm_stderr": 0.03878139888797611
    },
    "lambada_openai_mt_de": {
      "ppl": 136.7377568126123,
      "ppl_stderr": 8.137958498583759,
      "acc": 0.28177760527847856,
      "acc_stderr": 0.006267506327651741
    },
    "hendrycksTest_mt_fr": {
      "acc": 0.24841213832039521,
      "acc_stderr": 0.011482716927344988,
      "acc_norm": 0.27522935779816515,
      "acc_norm_stderr": 0.011869052432804589
    },
    "hendrycksTest-computer_security": {
      "acc": 0.34,
      "acc_stderr": 0.047609522856952365,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hellaswag_mt_fr": {
      "acc": 0.32715784964660527,
      "acc_stderr": 0.0048554734995366385,
      "acc_norm": 0.40372670807453415,
      "acc_norm_stderr": 0.005077649737505695
    },
    "hendrycksTest_mt_nl": {
      "acc": 0.2512212142358688,
      "acc_stderr": 0.0114612876394043,
      "acc_norm": 0.27424982554082344,
      "acc_norm_stderr": 0.011789496163308807
    },
    "hendrycksTest-marketing": {
      "acc": 0.3034188034188034,
      "acc_stderr": 0.030118210106942645,
      "acc_norm": 0.3076923076923077,
      "acc_norm_stderr": 0.030236389942173092
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.1948529411764706,
      "acc_stderr": 0.024060599423487424,
      "acc_norm": 0.22426470588235295,
      "acc_norm_stderr": 0.02533684856333237
    },
    "hendrycksTest-human_aging": {
      "acc": 0.24663677130044842,
      "acc_stderr": 0.028930413120910884,
      "acc_norm": 0.21524663677130046,
      "acc_norm_stderr": 0.027584066602208263
    },
    "hendrycksTest_mt_de": {
      "acc": 0.2484472049689441,
      "acc_stderr": 0.011355663739844521,
      "acc_norm": 0.2636300897170462,
      "acc_norm_stderr": 0.01157873794139459
    },
    "arc_challenge": {
      "acc": 0.2781569965870307,
      "acc_stderr": 0.013094469919538816,
      "acc_norm": 0.30119453924914674,
      "acc_norm_stderr": 0.013406741767847627
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.26382978723404255,
      "acc_stderr": 0.028809989854102956,
      "acc_norm": 0.19148936170212766,
      "acc_norm_stderr": 0.025722149992637798
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036844,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.19631901840490798,
      "acc_stderr": 0.031207970394709218,
      "acc_norm": 0.3006134969325153,
      "acc_norm_stderr": 0.0360251131880677
    },
    "truthfulqa_mc_mt_es": {
      "mc1": 0.2674271229404309,
      "mc1_stderr": 0.01576757970059163,
      "mc2": 0.41662554478019975,
      "mc2_stderr": 0.015334468018955208
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3101851851851852,
      "acc_stderr": 0.03154696285656628,
      "acc_norm": 0.3101851851851852,
      "acc_norm_stderr": 0.031546962856566295
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3103448275862069,
      "acc_stderr": 0.03855289616378947,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.039609335494512087
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-virology": {
      "acc": 0.2289156626506024,
      "acc_stderr": 0.03270745277352477,
      "acc_norm": 0.30120481927710846,
      "acc_norm_stderr": 0.035716092300534796
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.270935960591133,
      "acc_stderr": 0.031270907132976984,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.03178529710642749
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.0446196043338474
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2797427652733119,
      "acc_stderr": 0.0254942593506949,
      "acc_norm": 0.3086816720257235,
      "acc_norm_stderr": 0.02623696588115326
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.22695035460992907,
      "acc_stderr": 0.024987106365642976,
      "acc_norm": 0.20212765957446807,
      "acc_norm_stderr": 0.023956668237850236
    },
    "hendrycksTest-management": {
      "acc": 0.24271844660194175,
      "acc_stderr": 0.042450224863844935,
      "acc_norm": 0.30097087378640774,
      "acc_norm_stderr": 0.04541609446503946
    },
    "hendrycksTest-security_studies": {
      "acc": 0.34285714285714286,
      "acc_stderr": 0.030387262919547724,
      "acc_norm": 0.27346938775510204,
      "acc_norm_stderr": 0.02853556033712844
    },
    "hendrycksTest-global_facts": {
      "acc": 0.19,
      "acc_stderr": 0.03942772444036625,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.26011560693641617,
      "acc_stderr": 0.02361867831006937,
      "acc_norm": 0.2832369942196532,
      "acc_norm_stderr": 0.02425790170532337
    },
    "arc_challenge_mt_it": {
      "acc": 0.2172797262617622,
      "acc_stderr": 0.012066782166932079,
      "acc_norm": 0.2446535500427716,
      "acc_norm_stderr": 0.012578458921815744
    },
    "arc_challenge_mt_pt": {
      "acc": 0.2299145299145299,
      "acc_stderr": 0.012306807801472001,
      "acc_norm": 0.2692307692307692,
      "acc_norm_stderr": 0.012973143349154908
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.02534809746809785,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.027309140588230186
    },
    "truthfulqa_mc_mt_fr": {
      "mc1": 0.2503176620076239,
      "mc1_stderr": 0.015451587782682005,
      "mc2": 0.42622278329863655,
      "mc2_stderr": 0.015191532673480323
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.27419354838709675,
      "acc_stderr": 0.025378139970885203,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.026069362295335127
    },
    "truthfulqa_mc": {
      "mc1": 0.22031823745410037,
      "mc1_stderr": 0.014509045171487295,
      "mc2": 0.37595684250453254,
      "mc2_stderr": 0.013832540131836856
    },
    "hendrycksTest-anatomy": {
      "acc": 0.2,
      "acc_stderr": 0.034554737023254366,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.03633384414073463
    },
    "hendrycksTest_mt_es": {
      "acc": 0.2402730375426621,
      "acc_stderr": 0.011166337517762016,
      "acc_norm": 0.2552901023890785,
      "acc_norm_stderr": 0.011395673911071049
    },
    "hellaswag_mt_it": {
      "acc": 0.3188295442184271,
      "acc_stderr": 0.0048607375548380075,
      "acc_norm": 0.378875231154139,
      "acc_norm_stderr": 0.00505979036127035
    },
    "hendrycksTest-world_religions": {
      "acc": 0.30994152046783624,
      "acc_stderr": 0.035469769593931624,
      "acc_norm": 0.3567251461988304,
      "acc_norm_stderr": 0.03674013002860954
    },
    "piqa": {
      "acc": 0.7334058759521219,
      "acc_stderr": 0.010316749863541367,
      "acc_norm": 0.7328618063112078,
      "acc_norm_stderr": 0.010323440492612421
    },
    "truthfulqa_mc_mt_nl": {
      "mc1": 0.2585987261146497,
      "mc1_stderr": 0.01563802212323214,
      "mc2": 0.42526230255929703,
      "mc2_stderr": 0.015343850061146963
    },
    "lambada_openai_mt_fr": {
      "ppl": 94.58566153976989,
      "ppl_stderr": 5.410458025372398,
      "acc": 0.346206093537745,
      "acc_stderr": 0.006628264962716327
    },
    "hendrycksTest_mt_pt": {
      "acc": 0.2308214528173795,
      "acc_stderr": 0.010982412747968748,
      "acc_norm": 0.25797691785471827,
      "acc_norm_stderr": 0.011403685393902704
    },
    "lambada_openai_mt_es": {
      "ppl": 139.73942494400973,
      "ppl_stderr": 7.856809758495352,
      "acc": 0.29846691247816803,
      "acc_stderr": 0.006375059594075383
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2636363636363636,
      "acc_stderr": 0.04220224692971987,
      "acc_norm": 0.17272727272727273,
      "acc_norm_stderr": 0.03620691833929219
    },
    "hendrycksTest-college_biology": {
      "acc": 0.2361111111111111,
      "acc_stderr": 0.03551446610810826,
      "acc_norm": 0.2708333333333333,
      "acc_norm_stderr": 0.03716177437566017
    },
    "arc_challenge_mt_de": {
      "acc": 0.20701454234388367,
      "acc_stderr": 0.011855274590585597,
      "acc_norm": 0.22840034217279725,
      "acc_norm_stderr": 0.012283523666042812
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.2771392081736909,
      "acc_stderr": 0.016005636294122425,
      "acc_norm": 0.26947637292464877,
      "acc_norm_stderr": 0.015866243073215054
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.0340150671524904,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.037131580674819135
    }
  },
  "versions": {
    "hendrycksTest-college_physics": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "sciq": 0,
    "hendrycksTest-high_school_government_and_politics": 0,
    "truthfulqa_mc_mt_it": 1,
    "hellaswag_mt_pt": 0,
    "lambada_openai_mt_it": 0,
    "hendrycksTest-college_medicine": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-high_school_world_history": 0,
    "truthfulqa_mc_mt_pt": 1,
    "arc_easy": 0,
    "winogrande": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-medical_genetics": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-human_sexuality": 0,
    "arc_challenge_mt_nl": 0,
    "truthfulqa_mc_mt_de": 1,
    "hendrycksTest-econometrics": 0,
    "hellaswag": 0,
    "hendrycksTest-sociology": 0,
    "arc_challenge_mt_fr": 0,
    "hendrycksTest-formal_logic": 0,
    "hellaswag_mt_de": 0,
    "hendrycksTest_mt_it": 0,
    "hellaswag_mt_nl": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "hendrycksTest-machine_learning": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_us_history": 0,
    "lambada_openai": 0,
    "arc_challenge_mt_es": 0,
    "hellaswag_mt_es": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-professional_law": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-international_law": 0,
    "hendrycksTest-astronomy": 0,
    "lambada_openai_mt_de": 0,
    "hendrycksTest_mt_fr": 0,
    "hendrycksTest-computer_security": 0,
    "hellaswag_mt_fr": 0,
    "hendrycksTest_mt_nl": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-human_aging": 0,
    "hendrycksTest_mt_de": 0,
    "arc_challenge": 0,
    "hendrycksTest-conceptual_physics": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-logical_fallacies": 0,
    "truthfulqa_mc_mt_es": 1,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest-electrical_engineering": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-management": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-moral_disputes": 0,
    "arc_challenge_mt_it": 0,
    "arc_challenge_mt_pt": 0,
    "hendrycksTest-high_school_mathematics": 0,
    "truthfulqa_mc_mt_fr": 1,
    "hendrycksTest-high_school_biology": 0,
    "truthfulqa_mc": 1,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest_mt_es": 0,
    "hellaswag_mt_it": 0,
    "hendrycksTest-world_religions": 0,
    "piqa": 0,
    "truthfulqa_mc_mt_nl": 1,
    "lambada_openai_mt_fr": 0,
    "hendrycksTest_mt_pt": 0,
    "lambada_openai_mt_es": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-college_biology": 0,
    "arc_challenge_mt_de": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-high_school_european_history": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T,dtype=auto,trust_remote_code=True,use_fast=False",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}