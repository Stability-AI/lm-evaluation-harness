{
  "results": {
    "hendrycksTest-college_mathematics": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-sociology": {
      "acc": 0.5024875621890548,
      "acc_stderr": 0.0353549015013729,
      "acc_norm": 0.417910447761194,
      "acc_norm_stderr": 0.034875586404620636
    },
    "hendrycksTest-marketing": {
      "acc": 0.6324786324786325,
      "acc_stderr": 0.031585391577456365,
      "acc_norm": 0.6068376068376068,
      "acc_norm_stderr": 0.03199957924651047
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.35661764705882354,
      "acc_stderr": 0.02909720956841195,
      "acc_norm": 0.33088235294117646,
      "acc_norm_stderr": 0.02858270975389844
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252606
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956911
    },
    "truthfulqa_mc_mt_nl": {
      "mc1": 0.2585987261146497,
      "mc1_stderr": 0.01563802212323214,
      "mc2": 0.4163410489156683,
      "mc2_stderr": 0.01609743750101572
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.026719240783712166,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.02840653309060846
    },
    "lambada_openai_mt_es": {
      "ppl": 681.267452059233,
      "ppl_stderr": 42.92301916184958,
      "acc": 0.14671065398796818,
      "acc_stderr": 0.004929365951015958
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.03925523381052932,
      "acc_norm": 0.3618421052631579,
      "acc_norm_stderr": 0.03910525752849724
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.48091603053435117,
      "acc_stderr": 0.04382094705550989,
      "acc_norm": 0.3893129770992366,
      "acc_norm_stderr": 0.04276486542814591
    },
    "truthfulqa_mc": {
      "mc1": 0.23745410036719705,
      "mc1_stderr": 0.014896277441041836,
      "mc2": 0.3808989557142085,
      "mc2_stderr": 0.014337956476761941
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4,
      "acc_stderr": 0.04232073695151589,
      "acc_norm": 0.34814814814814815,
      "acc_norm_stderr": 0.041153246103369526
    },
    "hendrycksTest-international_law": {
      "acc": 0.4132231404958678,
      "acc_stderr": 0.04495087843548408,
      "acc_norm": 0.48760330578512395,
      "acc_norm_stderr": 0.04562951548180765
    },
    "truthfulqa_mc_mt_de": {
      "mc1": 0.2550761421319797,
      "mc1_stderr": 0.015538299767129315,
      "mc2": 0.4304951545047661,
      "mc2_stderr": 0.01582721588543576
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.40932642487046633,
      "acc_stderr": 0.03548608168860806,
      "acc_norm": 0.37823834196891193,
      "acc_norm_stderr": 0.03499807276193339
    },
    "hendrycksTest-nutrition": {
      "acc": 0.45751633986928103,
      "acc_stderr": 0.02852638345214264,
      "acc_norm": 0.47058823529411764,
      "acc_norm_stderr": 0.028580341065138296
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-computer_security": {
      "acc": 0.47,
      "acc_stderr": 0.050161355804659205,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620333
    },
    "piqa": {
      "acc": 0.7285092491838956,
      "acc_stderr": 0.010376251176596137,
      "acc_norm": 0.7301414581066377,
      "acc_norm_stderr": 0.010356595421852195
    },
    "hellaswag_mt_fr": {
      "acc": 0.3090597558363675,
      "acc_stderr": 0.004782310761015314,
      "acc_norm": 0.37491968301563505,
      "acc_norm_stderr": 0.005009949555524306
    },
    "hellaswag_mt_pt": {
      "acc": 0.3142268934879185,
      "acc_stderr": 0.00483234694914764,
      "acc_norm": 0.3640697800411746,
      "acc_norm_stderr": 0.005008910301895115
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808779,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.04755129616062947
    },
    "truthfulqa_mc_mt_fr": {
      "mc1": 0.2668360864040661,
      "mc1_stderr": 0.01577653221395634,
      "mc2": 0.41632862751410155,
      "mc2_stderr": 0.015472265046941467
    },
    "hellaswag_mt_it": {
      "acc": 0.300663548352007,
      "acc_stderr": 0.004782758632810981,
      "acc_norm": 0.35548787120635267,
      "acc_norm_stderr": 0.004992555866576597
    },
    "lambada_openai": {
      "ppl": 7.666642300410342,
      "ppl_stderr": 0.21774172266521855,
      "acc": 0.5715117407335533,
      "acc_stderr": 0.006894362498866278
    },
    "hendrycksTest_mt_de": {
      "acc": 0.2781228433402346,
      "acc_stderr": 0.011775129685523519,
      "acc_norm": 0.28847481021394067,
      "acc_norm_stderr": 0.01190597063660515
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.37962962962962965,
      "acc_stderr": 0.04691521224077742,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.029832808114796005,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.03010279378179119
    },
    "arc_challenge_mt_es": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.012322817579787067,
      "acc_norm": 0.27863247863247864,
      "acc_norm_stderr": 0.013112542415245575
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5302752293577981,
      "acc_stderr": 0.021397988604936965,
      "acc_norm": 0.41651376146788993,
      "acc_norm_stderr": 0.021136376504030885
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.0314471258167824,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.0317852971064275
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2569832402234637,
      "acc_stderr": 0.01461446582196632,
      "acc_norm": 0.23910614525139665,
      "acc_norm_stderr": 0.014265554192331149
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.040735243221471276,
      "acc_norm": 0.3253968253968254,
      "acc_norm_stderr": 0.041905964388711366
    },
    "hendrycksTest-professional_law": {
      "acc": 0.28683181225554105,
      "acc_stderr": 0.01155150478117693,
      "acc_norm": 0.30378096479791394,
      "acc_norm_stderr": 0.011745787720472464
    },
    "truthfulqa_mc_mt_es": {
      "mc1": 0.302915082382763,
      "mc1_stderr": 0.01636967975523945,
      "mc2": 0.44357809976038876,
      "mc2_stderr": 0.015954205618293176
    },
    "arc_challenge_mt_fr": {
      "acc": 0.2275449101796407,
      "acc_stderr": 0.012267293637033647,
      "acc_norm": 0.2831479897348161,
      "acc_norm_stderr": 0.013182569621225304
    },
    "hendrycksTest-management": {
      "acc": 0.5145631067961165,
      "acc_stderr": 0.04948637324026637,
      "acc_norm": 0.5048543689320388,
      "acc_norm_stderr": 0.049505043821289195
    },
    "hellaswag": {
      "acc": 0.45518820952001593,
      "acc_stderr": 0.00496970108106837,
      "acc_norm": 0.6027683728340968,
      "acc_norm_stderr": 0.004883246579496652
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3191489361702128,
      "acc_stderr": 0.030472973363380035,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.029241883869628817
    },
    "arc_challenge_mt_nl": {
      "acc": 0.20872540633019676,
      "acc_stderr": 0.011891313995077585,
      "acc_norm": 0.24037639007698888,
      "acc_norm_stderr": 0.01250327289928353
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-virology": {
      "acc": 0.3674698795180723,
      "acc_stderr": 0.03753267402120574,
      "acc_norm": 0.3373493975903614,
      "acc_norm_stderr": 0.0368078369072758
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.36243386243386244,
      "acc_stderr": 0.024757473902752045,
      "acc_norm": 0.3201058201058201,
      "acc_norm_stderr": 0.024026846392873506
    },
    "arc_easy": {
      "acc": 0.6388888888888888,
      "acc_stderr": 0.00985601342581124,
      "acc_norm": 0.5854377104377104,
      "acc_norm_stderr": 0.010108889212447783
    },
    "lambada_openai_mt_de": {
      "ppl": 431.9066680694402,
      "ppl_stderr": 28.55935306355161,
      "acc": 0.20201824180089267,
      "acc_stderr": 0.005593760245498454
    },
    "arc_challenge": {
      "acc": 0.3199658703071672,
      "acc_stderr": 0.013631345807016196,
      "acc_norm": 0.34726962457337884,
      "acc_norm_stderr": 0.01391303452962044
    },
    "lambada_openai_mt_it": {
      "ppl": 452.9751882131516,
      "ppl_stderr": 30.671110187802142,
      "acc": 0.24859305259072384,
      "acc_stderr": 0.006021354126051936
    },
    "truthfulqa_mc_mt_it": {
      "mc1": 0.2720306513409962,
      "mc1_stderr": 0.015913367447500514,
      "mc2": 0.42453759264387264,
      "mc2_stderr": 0.01589981748346217
    },
    "sciq": {
      "acc": 0.919,
      "acc_stderr": 0.008632121032140007,
      "acc_norm": 0.889,
      "acc_norm_stderr": 0.009938701010583726
    },
    "hendrycksTest_mt_it": {
      "acc": 0.26177285318559557,
      "acc_stderr": 0.011572418835304169,
      "acc_norm": 0.27354570637119113,
      "acc_norm_stderr": 0.011735076717674332
    },
    "hendrycksTest_mt_pt": {
      "acc": 0.27359131025118805,
      "acc_stderr": 0.011619508555886306,
      "acc_norm": 0.29938900203665986,
      "acc_norm_stderr": 0.011937200585140941
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.43548387096774194,
      "acc_stderr": 0.028206225591502734,
      "acc_norm": 0.4064516129032258,
      "acc_norm_stderr": 0.027941727346256304
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3006134969325153,
      "acc_stderr": 0.0360251131880677,
      "acc_norm": 0.3067484662576687,
      "acc_norm_stderr": 0.03623089915724147
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3877551020408163,
      "acc_stderr": 0.031192230726795656,
      "acc_norm": 0.2816326530612245,
      "acc_norm_stderr": 0.028795185574291293
    },
    "hendrycksTest_mt_nl": {
      "acc": 0.2882065596650384,
      "acc_stderr": 0.011968987469925032,
      "acc_norm": 0.29867411025819957,
      "acc_norm_stderr": 0.012094480426520325
    },
    "hendrycksTest-prehistory": {
      "acc": 0.38580246913580246,
      "acc_stderr": 0.027085401226132143,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.025407197798890162
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5159642401021711,
      "acc_stderr": 0.017870847506081738,
      "acc_norm": 0.45977011494252873,
      "acc_norm_stderr": 0.01782199409693354
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3729903536977492,
      "acc_stderr": 0.027466610213140112,
      "acc_norm": 0.3762057877813505,
      "acc_norm_stderr": 0.02751392568354943
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.30141843971631205,
      "acc_stderr": 0.02737412888263115,
      "acc_norm": 0.25886524822695034,
      "acc_norm_stderr": 0.026129572527180848
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3541666666666667,
      "acc_stderr": 0.039994111357535424,
      "acc_norm": 0.3194444444444444,
      "acc_norm_stderr": 0.038990736873573344
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.031968769891957786,
      "acc_norm": 0.3865546218487395,
      "acc_norm_stderr": 0.0316314580755238
    },
    "hendrycksTest-global_facts": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909284
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3769230769230769,
      "acc_stderr": 0.024570975364225995,
      "acc_norm": 0.34615384615384615,
      "acc_norm_stderr": 0.024121125416941183
    },
    "lambada_openai_mt_fr": {
      "ppl": 219.34763225733118,
      "ppl_stderr": 13.747791008739977,
      "acc": 0.27498544537162817,
      "acc_stderr": 0.006220714778345997
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4689655172413793,
      "acc_stderr": 0.04158632762097828,
      "acc_norm": 0.4206896551724138,
      "acc_norm_stderr": 0.0411391498118926
    },
    "arc_challenge_mt_it": {
      "acc": 0.20615911035072712,
      "acc_stderr": 0.011837134350507168,
      "acc_norm": 0.262617621899059,
      "acc_norm_stderr": 0.012876175520452835
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3627450980392157,
      "acc_stderr": 0.01945076843250551,
      "acc_norm": 0.3382352941176471,
      "acc_norm_stderr": 0.019139943748487036
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4345991561181435,
      "acc_stderr": 0.03226759995510144,
      "acc_norm": 0.4219409282700422,
      "acc_norm_stderr": 0.032148146302403695
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.4484848484848485,
      "acc_stderr": 0.038835659779569286,
      "acc_norm": 0.4121212121212121,
      "acc_norm_stderr": 0.03843566993588718
    },
    "winogrande": {
      "acc": 0.5880031570639306,
      "acc_stderr": 0.01383311285764593
    },
    "arc_challenge_mt_pt": {
      "acc": 0.2282051282051282,
      "acc_stderr": 0.012274572870357082,
      "acc_norm": 0.2658119658119658,
      "acc_norm_stderr": 0.01292062907954085
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3786127167630058,
      "acc_stderr": 0.026113749361310338,
      "acc_norm": 0.35260115606936415,
      "acc_norm_stderr": 0.02572280220089582
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.0316746870682898,
      "acc_norm": 0.35185185185185186,
      "acc_norm_stderr": 0.032568505702936464
    },
    "hendrycksTest_mt_fr": {
      "acc": 0.28722653493295697,
      "acc_stderr": 0.012024206181538944,
      "acc_norm": 0.3048694424841214,
      "acc_norm_stderr": 0.012233719306964734
    },
    "hendrycksTest_mt_es": {
      "acc": 0.2757679180887372,
      "acc_stderr": 0.011679929232937756,
      "acc_norm": 0.2955631399317406,
      "acc_norm_stderr": 0.01192547355732891
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.3509933774834437,
      "acc_stderr": 0.03896981964257375,
      "acc_norm": 0.304635761589404,
      "acc_norm_stderr": 0.03757949922943343
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.37572254335260113,
      "acc_stderr": 0.03692820767264867,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.0356760379963917
    },
    "truthfulqa_mc_mt_pt": {
      "mc1": 0.2906091370558376,
      "mc1_stderr": 0.016184901529011933,
      "mc2": 0.4305255045657521,
      "mc2_stderr": 0.01593557843105302
    },
    "hellaswag_mt_de": {
      "acc": 0.2939795046968403,
      "acc_stderr": 0.0047072462818428355,
      "acc_norm": 0.32749786507258755,
      "acc_norm_stderr": 0.00484898465977777
    },
    "hendrycksTest-abstract_algebra": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.042923469599092816
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.03811079669833531,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.03811079669833531
    },
    "arc_challenge_mt_de": {
      "acc": 0.21813515825491872,
      "acc_stderr": 0.012083903698128144,
      "acc_norm": 0.25919589392643283,
      "acc_norm_stderr": 0.012821662109963668
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5181818181818182,
      "acc_stderr": 0.04785964010794916,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.047093069786618966
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.040073418097558045,
      "acc_norm": 0.23214285714285715,
      "acc_norm_stderr": 0.04007341809755804
    },
    "hellaswag_mt_es": {
      "acc": 0.32419458075528057,
      "acc_stderr": 0.004834754228966938,
      "acc_norm": 0.3880947301045445,
      "acc_norm_stderr": 0.00503351840561876
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4292929292929293,
      "acc_stderr": 0.03526552724601199,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.035212249088415824
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.39705882352941174,
      "acc_stderr": 0.03434131164719128,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.033321399446680854
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4170403587443946,
      "acc_stderr": 0.03309266936071721,
      "acc_norm": 0.33183856502242154,
      "acc_norm_stderr": 0.03160295143776679
    },
    "hellaswag_mt_nl": {
      "acc": 0.29174311926605506,
      "acc_stderr": 0.004722766119718833,
      "acc_norm": 0.32401511063140853,
      "acc_norm_stderr": 0.004862413101925825
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.04142439719489363,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    }
  },
  "versions": {
    "hendrycksTest-college_mathematics": 0,
    "hendrycksTest-high_school_computer_science": 0,
    "hendrycksTest-sociology": 0,
    "hendrycksTest-marketing": 0,
    "hendrycksTest-professional_medicine": 0,
    "hendrycksTest-college_chemistry": 0,
    "hendrycksTest-medical_genetics": 0,
    "truthfulqa_mc_mt_nl": 1,
    "hendrycksTest-high_school_mathematics": 0,
    "lambada_openai_mt_es": 0,
    "hendrycksTest-astronomy": 0,
    "hendrycksTest-human_sexuality": 0,
    "truthfulqa_mc": 1,
    "hendrycksTest-anatomy": 0,
    "hendrycksTest-international_law": 0,
    "truthfulqa_mc_mt_de": 1,
    "hendrycksTest-high_school_government_and_politics": 0,
    "hendrycksTest-nutrition": 0,
    "hendrycksTest-college_computer_science": 0,
    "hendrycksTest-computer_security": 0,
    "piqa": 0,
    "hellaswag_mt_fr": 0,
    "hellaswag_mt_pt": 0,
    "hendrycksTest-college_physics": 0,
    "truthfulqa_mc_mt_fr": 1,
    "hellaswag_mt_it": 0,
    "lambada_openai": 0,
    "hendrycksTest_mt_de": 0,
    "hendrycksTest-jurisprudence": 0,
    "hendrycksTest-clinical_knowledge": 0,
    "arc_challenge_mt_es": 0,
    "hendrycksTest-us_foreign_policy": 0,
    "hendrycksTest-high_school_psychology": 0,
    "hendrycksTest-high_school_chemistry": 0,
    "hendrycksTest-moral_scenarios": 0,
    "hendrycksTest-formal_logic": 0,
    "hendrycksTest-professional_law": 0,
    "truthfulqa_mc_mt_es": 1,
    "arc_challenge_mt_fr": 0,
    "hendrycksTest-management": 0,
    "hellaswag": 0,
    "hendrycksTest-conceptual_physics": 0,
    "arc_challenge_mt_nl": 0,
    "hendrycksTest-business_ethics": 0,
    "hendrycksTest-virology": 0,
    "hendrycksTest-elementary_mathematics": 0,
    "arc_easy": 0,
    "lambada_openai_mt_de": 0,
    "arc_challenge": 0,
    "lambada_openai_mt_it": 0,
    "truthfulqa_mc_mt_it": 1,
    "sciq": 0,
    "hendrycksTest_mt_it": 0,
    "hendrycksTest_mt_pt": 0,
    "hendrycksTest-high_school_biology": 0,
    "hendrycksTest-logical_fallacies": 0,
    "hendrycksTest-security_studies": 0,
    "hendrycksTest_mt_nl": 0,
    "hendrycksTest-prehistory": 0,
    "hendrycksTest-miscellaneous": 0,
    "hendrycksTest-philosophy": 0,
    "hendrycksTest-professional_accounting": 0,
    "hendrycksTest-college_biology": 0,
    "hendrycksTest-high_school_microeconomics": 0,
    "hendrycksTest-global_facts": 0,
    "hendrycksTest-high_school_macroeconomics": 0,
    "lambada_openai_mt_fr": 0,
    "hendrycksTest-electrical_engineering": 0,
    "arc_challenge_mt_it": 0,
    "hendrycksTest-professional_psychology": 0,
    "hendrycksTest-high_school_world_history": 0,
    "hendrycksTest-high_school_european_history": 0,
    "winogrande": 0,
    "arc_challenge_mt_pt": 0,
    "hendrycksTest-moral_disputes": 0,
    "hendrycksTest-high_school_statistics": 0,
    "hendrycksTest_mt_fr": 0,
    "hendrycksTest_mt_es": 0,
    "hendrycksTest-high_school_physics": 0,
    "hendrycksTest-college_medicine": 0,
    "truthfulqa_mc_mt_pt": 1,
    "hellaswag_mt_de": 0,
    "hendrycksTest-abstract_algebra": 0,
    "hendrycksTest-world_religions": 0,
    "arc_challenge_mt_de": 0,
    "hendrycksTest-public_relations": 0,
    "hendrycksTest-machine_learning": 0,
    "hellaswag_mt_es": 0,
    "hendrycksTest-high_school_geography": 0,
    "hendrycksTest-high_school_us_history": 0,
    "hendrycksTest-human_aging": 0,
    "hellaswag_mt_nl": 0,
    "hendrycksTest-econometrics": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=Qwen/Qwen-1_8b,dtype=auto,trust_remote_code=True,use_fast=False",
    "num_fewshot": 0,
    "batch_size": 8,
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}