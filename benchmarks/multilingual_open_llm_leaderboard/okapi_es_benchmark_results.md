# Results
|                Model                 | Average | ARC Challenge (es)<br>(acc_norm) | HellaSwag (es)<br>(acc_norm) | MMLU (es)<br>(acc) | TruthfulQA (es)<br>(mc2) |
| ------------------------------------ | ------: | -------------------------------: | ---------------------------: | -----------------: | -----------------------: |
| mistralai/Mistral-7B-v0.1            |   46.53 |                            40.00 |                        64.10 |              37.27 |                    44.75 |
| /fsx/dakota/ckpts/spanish3b_hf_60000 |   41.04 |                            35.64 |                        61.43 |              26.76 |                    40.35 |
| meta-llama/Llama-2-7b-hf             |   40.11 |                            34.19 |                        56.76 |              28.46 |                    41.03 |
| /fsx/dakota/ckpts/spanish3b_hf_30000 |   39.57 |                            32.82 |                        59.42 |              25.94 |                    40.08 |
| stabilityai/stablelm-3b-4e1t         |   39.03 |                            32.48 |                        53.22 |              28.26 |                    42.18 |
