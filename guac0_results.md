# Results
|                   Model                   | Average | ARC Challenge✱ | ARC Easy✱ | BoolQ | HellaSwag✱ | LAMBADA OpenAI | OpenBookQA | PIQA  | SciQ | Winogrande |
| ----------------------------------------- | ------: | -------------: | --------: | ----: | ---------: | -------------: | ---------: | ----: | ---: | ---------: |
| meta-llama/Llama-2-13b-hf                 |   71.63 |          49.23 |     77.61 | 80.52 |      79.36 |          76.77 |       35.4 | 79.05 | 94.5 |      72.22 |
| meta-llama/Llama-2-7b-hf                  |   68.91 |          46.16 |     74.54 | 77.74 |      75.94 |          73.47 |       31.4 | 77.75 | 93.6 |      69.61 |
| huggyllama/llama-7b                       |   68.88 |          44.62 |     72.85 | 75.05 |      76.22 |          73.55 |       34.4 | 78.67 | 94.6 |      69.93 |
| Qwen/Qwen-7B                              |   68.09 |          49.15 |     65.19 | 74.56 |      88.85 |          69.67 |       32.2 | 73.99 | 93.2 |      65.98 |
| Qwen/Qwen-7B-Chat                         |   67.43 |          46.67 |     64.48 | 71.68 |      84.97 |          65.48 |       35.6 | 78.73 | 90.7 |      68.59 |
| mosaicml/mpt-7b                           |   66.97 |          41.89 |     70.03 | 73.94 |      76.17 |          68.64 |       31.4 | 78.89 | 93.7 |      68.03 |
| baichuan-inc/Baichuan2-7B-Base            |   66.79 |          43.17 |     72.81 | 73.09 |      72.29 |          70.99 |       30.4 | 76.17 | 94.6 |      67.56 |
| stabilityai/stablelm-base-alpha-7b-v2     |   66.66 |          40.53 |     69.11 | 70.31 |      74.27 |          74.19 |       30.4 | 78.45 | 93.9 |      68.82 |
| openlm-research/open_llama_7b_v2          |   66.46 |          42.41 |     69.65 | 71.41 |      74.65 |          71.05 |       30.2 | 79.16 | 93.8 |      65.82 |
| microsoft/phi-1_5                         |   65.64 |          48.04 |     73.15 | 74.53 |      62.62 |          52.75 |       37.6 | 76.33 | 93.2 |      72.53 |
| togethercomputer/RedPajama-INCITE-7B-Base |   64.91 |          39.42 |     69.19 | 70.76 |      70.33 |          71.34 |       29.0 | 77.15 | 92.7 |      64.33 |
| cerebras/btlm-3b-8k-base                  |   63.52 |          37.63 |     67.09 | 69.63 |      69.78 |          66.23 |       27.6 | 75.84 | 92.9 |      64.96 |
| EleutherAI/pythia-12b                     |   62.33 |          35.07 |     63.72 | 67.31 |      67.38 |          70.64 |       26.4 | 76.28 | 90.2 |      64.01 |
| openlm-research/open_llama_3b_v2          |   62.22 |          36.09 |     63.51 | 65.69 |      69.99 |          66.74 |       26.0 | 76.66 | 92.4 |      62.90 |
| stabilityai/stablelm-base-alpha-3b-v2     |   62.04 |          35.07 |     63.26 | 64.56 |      68.58 |          70.25 |       26.4 | 76.01 | 92.1 |      62.12 |
| facebook/opt-6.7b                         |   61.69 |          34.81 |     60.14 | 66.02 |      67.20 |          67.65 |       27.6 | 76.33 | 90.1 |      65.35 |
| bigscience/bloom-7b1                      |   58.47 |          33.53 |     57.37 | 62.84 |      62.29 |          57.56 |       25.2 | 72.74 | 90.1 |      64.64 |
| EleutherAI/pythia-2.8b-deduped            |   58.34 |          32.94 |     59.09 | 64.13 |      59.44 |          65.15 |       23.8 | 74.10 | 88.2 |      58.25 |
| facebook/opt-2.7b                         |   57.31 |          31.31 |     54.29 | 60.34 |      60.60 |          63.57 |       25.0 | 73.83 | 85.8 |      61.01 |
| bigscience/bloom-3b                       |   54.62 |          30.38 |     53.28 | 61.71 |      54.53 |          51.74 |       21.8 | 70.57 | 89.1 |      58.48 |
| stabilityai/stablelm-base-alpha-7b        |   49.63 |          27.05 |     44.87 | 60.06 |      41.22 |          55.11 |       21.4 | 66.76 | 80.1 |      50.12 |
| stabilityai/stablelm-base-alpha-3b        |   45.63 |          25.77 |     42.05 | 57.65 |      38.31 |          41.72 |       17.0 | 63.82 | 71.7 |      52.64 |
