# Results
|                        Model                        | Average | ARC Challenge✱ | ARC Easy✱ | BoolQ | HellaSwag✱ | LAMBADA OpenAI | OpenBookQA | PIQA  | SciQ | Winogrande |
| --------------------------------------------------- | ------: | -------------: | --------: | ----: | ---------: | -------------: | ---------: | ----: | ---: | ---------: |
| stabilityai/stablelm-2-12b                          |   73.82 |          54.69 |     80.47 | 87.25 |      81.77 |          74.19 |       35.0 | 80.14 | 96.0 |      74.90 |
| internlm/internlm2-base-20b                         |   73.21 |          57.76 |     80.05 | 82.75 |      80.06 |          73.45 |       33.8 | 80.63 | 96.4 |      73.95 |
| mistralai/Mistral-7B-v0.1                           |   73.00 |          54.01 |     79.50 | 83.73 |      81.10 |          75.74 |       32.4 | 80.63 | 95.9 |      73.95 |
| meta-llama/Meta-Llama-3-8B                          |   72.29 |          53.33 |     77.69 | 81.19 |      79.14 |          75.61 |       34.6 | 79.60 | 96.4 |      73.09 |
| internlm/internlm-20b                               |   72.23 |          54.44 |     80.81 | 82.05 |      79.73 |          70.77 |       31.8 | 79.87 | 95.8 |      74.82 |
| google/gemma-7b                                     |   71.78 |          52.90 |     80.56 | 82.94 |      80.65 |          67.48 |       32.4 | 79.71 | 95.4 |      74.03 |
| 01-ai/Yi-9B                                         |   71.76 |          54.69 |     79.92 | 85.47 |      75.92 |          68.45 |       32.8 | 78.45 | 97.4 |      72.69 |
| meta-llama/Llama-2-13b-hf                           |   71.73 |          48.81 |     76.52 | 82.14 |      79.63 |          76.54 |       34.6 | 79.49 | 95.2 |      72.61 |
| xverse/XVERSE-MoE-A4.2B                             |   71.66 |          57.42 |     83.16 | 76.61 |      77.06 |          69.67 |       33.6 | 79.76 | 96.7 |      70.96 |
| tiiuae/falcon-11B                                   |   71.41 |          50.34 |     68.10 | 87.34 |      82.07 |          73.78 |       32.2 | 80.30 | 94.8 |      73.80 |
| microsoft/phi-2                                     |   71.35 |          54.10 |     78.20 | 83.39 |      73.83 |          62.70 |       40.4 | 78.78 | 95.0 |      75.77 |
| qwen/qwen-14b                                       |   70.40 |          47.27 |     70.58 | 86.51 |      81.31 |          72.87 |       33.4 | 79.65 | 94.9 |      67.09 |
| Qwen/Qwen1.5-14B                                    |   70.05 |          47.01 |     68.14 | 85.69 |      79.48 |          72.02 |       32.6 | 79.38 | 95.0 |      71.11 |
| meta-llama/Llama-2-7b-hf                            |   68.91 |          46.16 |     74.54 | 77.74 |      75.94 |          73.47 |       31.4 | 77.75 | 93.6 |      69.61 |
| huggyllama/llama-7b                                 |   68.88 |          44.62 |     72.85 | 75.05 |      76.22 |          73.55 |       34.4 | 78.67 | 94.6 |      69.93 |
| Qwen/Qwen-7B                                        |   68.09 |          49.15 |     65.19 | 74.56 |      88.85 |          69.67 |       32.2 | 73.99 | 93.2 |      65.98 |
| tiiuae/falcon-7b                                    |   67.81 |          43.69 |     70.79 | 73.55 |      76.35 |          74.56 |       30.6 | 79.49 | 94.0 |      67.25 |
| Qwen/Qwen-7B-Chat                                   |   67.43 |          46.67 |     64.48 | 71.68 |      84.97 |          65.48 |       35.6 | 78.73 | 90.7 |      68.59 |
| mosaicml/mpt-7b                                     |   66.97 |          41.89 |     70.03 | 73.94 |      76.17 |          68.64 |       31.4 | 78.89 | 93.7 |      68.03 |
| baichuan-inc/Baichuan2-7B-Base                      |   66.79 |          43.17 |     72.81 | 73.09 |      72.29 |          70.99 |       30.4 | 76.17 | 94.6 |      67.56 |
| stabilityai/stablelm-base-alpha-7b-v2               |   66.66 |          40.53 |     69.11 | 70.31 |      74.27 |          74.19 |       30.4 | 78.45 | 93.9 |      68.82 |
| stablelm-3b-4e1t                                    |   66.65 |          40.02 |     67.72 | 75.63 |      73.90 |          70.64 |       31.4 | 79.22 | 94.8 |      66.54 |
| openlm-research/open_llama_7b_v2                    |   66.46 |          42.41 |     69.65 | 71.41 |      74.65 |          71.05 |       30.2 | 79.16 | 93.8 |      65.82 |
| microsoft/phi-1_5                                   |   65.64 |          48.04 |     73.15 | 74.53 |      62.62 |          52.75 |       37.6 | 76.33 | 93.2 |      72.53 |
| EleutherAI/gpt-neox-20B                             |   65.42 |          40.78 |     68.69 | 69.48 |      71.43 |          71.98 |       29.8 | 77.42 | 93.1 |      66.14 |
| togethercomputer/RedPajama-INCITE-7B-Base           |   64.91 |          39.42 |     69.19 | 70.76 |      70.33 |          71.34 |       29.0 | 77.15 | 92.7 |      64.33 |
| stabilityai/stablelm-2-1_6b                         |   64.57 |          39.08 |     68.35 | 74.80 |      69.00 |          66.25 |       27.8 | 76.28 | 95.5 |      64.09 |
| h2oai/h2o-danube2-1.8b-base                         |   64.56 |          40.19 |     68.69 | 72.48 |      72.44 |          63.73 |       29.2 | 76.12 | 94.0 |      64.17 |
| stabilityai/stablelm-zephyr-3b                      |   64.21 |          42.32 |     61.32 | 82.23 |      71.14 |          60.10 |       29.0 | 75.68 | 92.1 |      64.01 |
| cerebras/btlm-3b-8k-base                            |   63.52 |          37.63 |     67.09 | 69.63 |      69.78 |          66.23 |       27.6 | 75.84 | 92.9 |      64.96 |
| EleutherAI/pythia-12b                               |   62.33 |          35.07 |     63.72 | 67.31 |      67.38 |          70.64 |       26.4 | 76.28 | 90.2 |      64.01 |
| openlm-research/open_llama_3b_v2                    |   62.22 |          36.09 |     63.51 | 65.69 |      69.99 |          66.74 |       26.0 | 76.66 | 92.4 |      62.90 |
| stabilityai/stablelm-base-alpha-3b-v2               |   62.04 |          35.07 |     63.26 | 64.56 |      68.58 |          70.25 |       26.4 | 76.01 | 92.1 |      62.12 |
| facebook/opt-6.7b                                   |   61.69 |          34.81 |     60.14 | 66.02 |      67.20 |          67.65 |       27.6 | 76.33 | 90.1 |      65.35 |
| EleutherAI/pythia-6.9b                              |   60.29 |          35.32 |     61.07 | 64.01 |      63.88 |          67.01 |       25.8 | 75.08 | 89.8 |      60.62 |
| bigscience/bloom-7b1                                |   58.47 |          33.53 |     57.37 | 62.84 |      62.29 |          57.56 |       25.2 | 72.74 | 90.1 |      64.64 |
| Qwen/Qwen-1_8B                                      |   58.41 |          34.73 |     58.54 | 65.87 |      60.28 |          57.15 |       25.6 | 72.85 | 91.9 |      58.80 |
| EleutherAI/pythia-2.8b-deduped                      |   58.34 |          32.94 |     59.09 | 64.13 |      59.44 |          65.15 |       23.8 | 74.10 | 88.2 |      58.25 |
| tiiuae/falcon-rw-1b                                 |   57.64 |          32.42 |     57.49 | 61.90 |      61.60 |          55.02 |       24.4 | 75.24 | 89.7 |      61.01 |
| TinyLlama/TinyLlama-1.1B-Chat-v1.0                  |   57.50 |          32.76 |     54.25 | 60.98 |      60.41 |          60.99 |       25.4 | 74.43 | 88.2 |      60.06 |
| facebook/opt-2.7b                                   |   57.31 |          31.31 |     54.29 | 60.34 |      60.60 |          63.57 |       25.0 | 73.83 | 85.8 |      61.01 |
| TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T |   55.97 |          30.38 |     55.22 | 56.94 |      59.27 |          58.65 |       21.8 | 73.29 | 88.8 |      59.35 |
| bigscience/bloom-3b                                 |   54.62 |          30.38 |     53.28 | 61.71 |      54.53 |          51.74 |       21.8 | 70.57 | 89.1 |      58.48 |
| stabilityai/stablelm-base-alpha-7b                  |   49.63 |          27.05 |     44.87 | 60.06 |      41.22 |          55.11 |       21.4 | 66.76 | 80.1 |      50.12 |
| stabilityai/stablelm-base-alpha-3b                  |   45.63 |          25.77 |     42.05 | 57.65 |      38.31 |          41.72 |       17.0 | 63.82 | 71.7 |      52.64 |
