# Results
|                        Model                        | Average | ARC Challenge✱ | ARC Easy✱ | BoolQ | HellaSwag✱ | LAMBADA OpenAI | OpenBookQA | PIQA  | SciQ | SocialIQA | Winogrande |
| --------------------------------------------------- | ------: | -------------: | --------: | ----: | ---------: | -------------: | ---------: | ----: | ---: | --------: | ---------: |
| mistralai/Mistral-Nemo-Base-2407                    |   71.53 |          58.02 |     81.52 | 85.26 |      82.76 |          78.09 |       36.2 | 81.39 | 96.5 |     42.63 |      72.93 |
| stabilityai/stablelm-2-12b                          |   70.83 |          54.69 |     80.47 | 87.25 |      81.77 |          74.19 |       35.0 | 80.14 | 96.0 |     43.86 |      74.90 |
| internlm/internlm2-base-20b                         |   70.31 |          57.76 |     80.05 | 82.75 |      80.06 |          73.45 |       33.8 | 80.63 | 96.4 |     44.27 |      73.95 |
| apple/DCLM-7B                                       |   70.25 |          55.38 |     81.14 | 80.49 |      79.08 |          75.12 |       36.0 | 79.98 | 96.6 |     45.19 |      73.48 |
| microsoft/phi-2                                     |   69.98 |          54.10 |     78.20 | 83.39 |      73.83 |          62.70 |       40.4 | 78.78 | 95.0 |     57.68 |      75.77 |
| mistralai/Mistral-7B-v0.1                           |   69.98 |          54.01 |     79.50 | 83.73 |      81.10 |          75.74 |       32.4 | 80.63 | 95.9 |     42.84 |      73.95 |
| internlm/internlm-20b                               |   69.83 |          54.44 |     80.81 | 82.05 |      79.73 |          70.77 |       31.8 | 79.87 | 95.8 |     48.21 |      74.82 |
| meta-llama/Meta-Llama-3-8B                          |   69.32 |          53.33 |     77.69 | 81.19 |      79.14 |          75.61 |       34.6 | 79.60 | 96.4 |     42.58 |      73.09 |
| 01-ai/Yi-9B                                         |   69.26 |          54.69 |     79.92 | 85.47 |      75.92 |          68.45 |       32.8 | 78.45 | 97.4 |     46.83 |      72.69 |
| google/gemma-7b                                     |   69.03 |          52.90 |     80.56 | 82.94 |      80.65 |          67.48 |       32.4 | 79.71 | 95.4 |     44.27 |      74.03 |
| meta-llama/Llama-2-13b-hf                           |   68.90 |          48.81 |     76.52 | 82.14 |      79.63 |          76.54 |       34.6 | 79.49 | 95.2 |     43.50 |      72.61 |
| xverse/XVERSE-MoE-A4.2B                             |   68.75 |          57.42 |     83.16 | 76.61 |      77.06 |          69.67 |       33.6 | 79.76 | 96.7 |     42.53 |      70.96 |
| tiiuae/falcon-11B                                   |   68.67 |          50.34 |     68.10 | 87.34 |      82.07 |          73.78 |       32.2 | 80.30 | 94.8 |     43.96 |      73.80 |
| qwen/qwen-14b                                       |   67.70 |          47.27 |     70.58 | 86.51 |      81.31 |          72.87 |       33.4 | 79.65 | 94.9 |     43.45 |      67.09 |
| Qwen/Qwen1.5-14B                                    |   67.49 |          47.01 |     68.14 | 85.69 |      79.48 |          72.02 |       32.6 | 79.38 | 95.0 |     44.47 |      71.11 |
| h2oai/h2o-danube3-4b-base                           |   67.34 |          48.98 |     76.09 | 77.86 |      78.68 |          68.27 |       34.4 | 79.87 | 95.2 |     45.50 |      68.51 |
| allenai/OLMoE-1B-7B-0924                            |   66.82 |          48.63 |     76.09 | 74.71 |      76.96 |          72.79 |       33.2 | 80.20 | 94.7 |     42.22 |      68.67 |
| huggyllama/llama-7b                                 |   66.47 |          44.62 |     72.85 | 75.05 |      76.22 |          73.55 |       34.4 | 78.67 | 94.6 |     44.83 |      69.93 |
| meta-llama/Llama-2-7b-hf                            |   66.37 |          46.16 |     74.54 | 77.74 |      75.94 |          73.47 |       31.4 | 77.75 | 93.6 |     43.50 |      69.61 |
| Qwen/Qwen-7B                                        |   66.22 |          49.15 |     65.19 | 74.56 |      88.85 |          69.67 |       32.2 | 73.99 | 93.2 |     49.44 |      65.98 |
| Qwen/Qwen1.5-MoE-A2.7B                              |   65.86 |          44.45 |     69.11 | 79.72 |      77.29 |          71.41 |       31.2 | 79.87 | 93.8 |     42.73 |      69.06 |
| Qwen/Qwen-7B-Chat                                   |   65.39 |          46.67 |     64.48 | 71.68 |      84.97 |          65.48 |       35.6 | 78.73 | 90.7 |     47.03 |      68.59 |
| tiiuae/falcon-7b                                    |   65.23 |          43.69 |     70.79 | 73.55 |      76.35 |          74.56 |       30.6 | 79.49 | 94.0 |     42.07 |      67.25 |
| mosaicml/mpt-7b                                     |   64.78 |          41.89 |     70.03 | 73.94 |      76.17 |          68.64 |       31.4 | 78.89 | 93.7 |     45.14 |      68.03 |
| microsoft/phi-1_5                                   |   64.61 |          48.04 |     73.15 | 74.53 |      62.62 |          52.75 |       37.6 | 76.33 | 93.2 |     55.37 |      72.53 |
| baichuan-inc/Baichuan2-7B-Base                      |   64.28 |          43.17 |     72.81 | 73.09 |      72.29 |          70.99 |       30.4 | 76.17 | 94.6 |     41.76 |      67.56 |
| stabilityai/stablelm-base-alpha-7b-v2               |   64.24 |          40.53 |     69.11 | 70.31 |      74.27 |          74.19 |       30.4 | 78.45 | 93.9 |     42.43 |      68.82 |
| stablelm-3b-4e1t                                    |   64.18 |          40.02 |     67.72 | 75.63 |      73.90 |          70.64 |       31.4 | 79.22 | 94.8 |     41.91 |      66.54 |
| openlm-research/open_llama_7b_v2                    |   64.01 |          42.41 |     69.65 | 71.41 |      74.65 |          71.05 |       30.2 | 79.16 | 93.8 |     41.97 |      65.82 |
| EleutherAI/gpt-neox-20B                             |   63.08 |          40.78 |     68.69 | 69.48 |      71.43 |          71.98 |       29.8 | 77.42 | 93.1 |     41.97 |      66.14 |
| togethercomputer/RedPajama-INCITE-7B-Base           |   62.68 |          39.42 |     69.19 | 70.76 |      70.33 |          71.34 |       29.0 | 77.15 | 92.7 |     42.58 |      64.33 |
| stabilityai/stablelm-zephyr-3b                      |   62.48 |          42.32 |     61.32 | 82.23 |      71.14 |          60.10 |       29.0 | 75.68 | 92.1 |     46.93 |      64.01 |
| h2oai/h2o-danube2-1.8b-base                         |   62.36 |          40.19 |     68.69 | 72.48 |      72.44 |          63.73 |       29.2 | 76.12 | 94.0 |     42.58 |      64.17 |
| stabilityai/stablelm-2-1_6b                         |   62.23 |          39.08 |     68.35 | 74.80 |      69.00 |          66.25 |       27.8 | 76.28 | 95.5 |     41.20 |      64.09 |
| cerebras/btlm-3b-8k-base                            |   61.44 |          37.63 |     67.09 | 69.63 |      69.78 |          66.23 |       27.6 | 75.84 | 92.9 |     42.78 |      64.96 |
| EleutherAI/pythia-12b                               |   60.30 |          35.07 |     63.72 | 67.31 |      67.38 |          70.64 |       26.4 | 76.28 | 90.2 |     42.02 |      64.01 |
| openlm-research/open_llama_3b_v2                    |   60.12 |          36.09 |     63.51 | 65.69 |      69.99 |          66.74 |       26.0 | 76.66 | 92.4 |     41.20 |      62.90 |
| stabilityai/stablelm-base-alpha-3b-v2               |   60.08 |          35.07 |     63.26 | 64.56 |      68.58 |          70.25 |       26.4 | 76.01 | 92.1 |     42.48 |      62.12 |
| facebook/opt-6.7b                                   |   59.78 |          34.81 |     60.14 | 66.02 |      67.20 |          67.65 |       27.6 | 76.33 | 90.1 |     42.63 |      65.35 |
| EleutherAI/pythia-6.9b                              |   58.33 |          35.32 |     61.07 | 64.01 |      63.88 |          67.01 |       25.8 | 75.08 | 89.8 |     40.74 |      60.62 |
| bigscience/bloom-7b1                                |   56.84 |          33.53 |     57.37 | 62.84 |      62.29 |          57.56 |       25.2 | 72.74 | 90.1 |     42.12 |      64.64 |
| Qwen/Qwen-1_8B                                      |   56.77 |          34.73 |     58.54 | 65.87 |      60.28 |          57.15 |       25.6 | 72.85 | 91.9 |     41.97 |      58.80 |
| EleutherAI/pythia-2.8b-deduped                      |   56.60 |          32.94 |     59.09 | 64.13 |      59.44 |          65.15 |       23.8 | 74.10 | 88.2 |     40.94 |      58.25 |
| tiiuae/falcon-rw-1b                                 |   55.85 |          32.42 |     57.49 | 61.90 |      61.60 |          55.02 |       24.4 | 75.24 | 89.7 |     39.71 |      61.01 |
| TinyLlama/TinyLlama-1.1B-Chat-v1.0                  |   55.82 |          32.76 |     54.25 | 60.98 |      60.41 |          60.99 |       25.4 | 74.43 | 88.2 |     40.69 |      60.06 |
| facebook/opt-2.7b                                   |   55.66 |          31.31 |     54.29 | 60.34 |      60.60 |          63.57 |       25.0 | 73.83 | 85.8 |     40.84 |      61.01 |
| TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T |   54.31 |          30.38 |     55.22 | 56.94 |      59.27 |          58.65 |       21.8 | 73.29 | 88.8 |     39.46 |      59.35 |
| bigscience/bloom-3b                                 |   53.18 |          30.38 |     53.28 | 61.71 |      54.53 |          51.74 |       21.8 | 70.57 | 89.1 |     40.17 |      58.48 |
| stabilityai/stablelm-base-alpha-7b                  |   48.61 |          27.05 |     44.87 | 60.06 |      41.22 |          55.11 |       21.4 | 66.76 | 80.1 |     39.46 |      50.12 |
| stabilityai/stablelm-base-alpha-3b                  |   44.63 |          25.77 |     42.05 | 57.65 |      38.31 |          41.72 |       17.0 | 63.82 | 71.7 |     35.62 |      52.64 |
| mistralai/Mistral-Nemo-Base-2407                    |   41.86 |          43.34 |     48.86 | 37.83 |      76.74 |           0.00 |       22.6 | 76.82 | 12.2 |     37.05 |      63.14 |
